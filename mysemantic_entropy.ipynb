{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyNZd8/c/M+Lk/qWzVtGHjL0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Reemaalt/Detection-of-Hallucination-in-Arabic/blob/main/mysemantic_entropy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Estimate the probability of each cluster.\n",
        "- Use Monte Carlo integration to compute semantic entropy."
      ],
      "metadata": {
        "id": "e6i8_59fWv9q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "from google.colab import files"
      ],
      "metadata": {
        "id": "Xj6HD8gwVBMK"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ZctZYrj3GIh",
        "outputId": "bdb06dea-c830-4e00-92f8-28378c087c2e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 100 questions with clustered answers.\n"
          ]
        }
      ],
      "source": [
        "# Load clustered responses\n",
        "file_path = \"entailment_clusters_Llama3.1-mlqa.json\"\n",
        "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    clustered_data = json.load(f)\n",
        "\n",
        "print(f\"Loaded {len(clustered_data)} questions with clustered answers.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#since Since we dont have log likelihoods, estimate cluster probabilities by assigning uniform probability\n",
        "def compute_cluster_uniform_probabilities(clusters):\n",
        "    total_responses = sum(len(cluster) for cluster in clusters)\n",
        "    probabilities = [len(cluster) / total_responses for cluster in clusters]\n",
        "    return probabilities\n"
      ],
      "metadata": {
        "id": "m6EB1MWEXKpM"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instead of assuming equal probability, the function:\n",
        "\n",
        "1.   Converts log probabilities to normal\n",
        "  probabilities using np.exp(log_prob).\n",
        "2.   Sums the probabilities within each cluster.\n",
        "3. Normalizes the probabilities so they sum to 1.\n"
      ],
      "metadata": {
        "id": "U9fKPVybUFIL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_cluster_log_probabilities(clusters, response_log_probs):\n",
        "    \"\"\"\n",
        "    Computes cluster probabilities by summing the exponentiated log probabilities of responses in each cluster.\n",
        "\n",
        "    Parameters:\n",
        "    - clusters: List of clusters, where each cluster is a list of response indices.\n",
        "    - response_log_probs: List containing log probabilities log(p(s|x)) for each response.\n",
        "\n",
        "    \"\"\"\n",
        "    cluster_probs = []\n",
        "\n",
        "    for cluster in clusters:\n",
        "        # Compute probability of each cluster by summing p(s|x) = exp(log_prob)\n",
        "        cluster_prob = sum(np.exp(response_log_probs[idx]) for idx in cluster)\n",
        "        cluster_probs.append(cluster_prob)\n",
        "\n",
        "    # Normalize to ensure probabilities sum to 1\n",
        "    total_prob = sum(cluster_probs)\n",
        "    return [prob / total_prob for prob in cluster_probs] if total_prob > 0 else cluster_probs\n"
      ],
      "metadata": {
        "id": "kon63tgWTmmV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#To retrieve log probabilities from the LLM at generation time :/\n",
        "\n",
        "\n",
        "#Compute log_softmax() over each generated token. Take the highest probability token at each position and sum the log probabilities.\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "# Load model and tokenizer\n",
        "model_name = \"aubmindlab/bert-base-arabic\"  # Replace with your Arabic LLM\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "def generate_responses_with_probs(prompt, num_responses=10):\n",
        "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
        "\n",
        "    outputs = model.generate(\n",
        "        input_ids,\n",
        "        max_length=100,\n",
        "        do_sample=True,\n",
        "        num_return_sequences=num_responses,\n",
        "        return_dict_in_generate=True,\n",
        "        output_scores=True\n",
        "    )\n",
        "\n",
        "    generated_texts = []\n",
        "    log_probs = []\n",
        "\n",
        "    for seq_idx in range(num_responses):\n",
        "        decoded_text = tokenizer.decode(outputs.sequences[seq_idx], skip_special_tokens=True)\n",
        "        generated_texts.append(decoded_text)\n",
        "\n",
        "        # Extract log probabilities\n",
        "        token_scores = outputs.scores  # List of logit tensors for each token position\n",
        "        log_prob_sum = sum(torch.log_softmax(scores, dim=-1)[seq_idx].max().item() for scores in token_scores)\n",
        "        log_probs.append(log_prob_sum)\n",
        "\n",
        "    return generated_texts, log_probs\n",
        "\n",
        "# Store responses and log probabilities\n",
        "generated_data = [{\"response\": resp, \"log_prob\": prob} for resp, prob in zip(responses, probabilities)]\n",
        "print(generated_data)\n"
      ],
      "metadata": {
        "id": "nAiggbGASqo-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#use Monte Carlo approximation based on Equation (3)\n",
        "def compute_semantic_entropy(probabilities):\n",
        "    probabilities = np.array(probabilities)\n",
        "    entropy = -np.sum(probabilities * np.log(probabilities))\n",
        "    return entropy\n"
      ],
      "metadata": {
        "id": "85_f8N40XbPP"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Compute SE\n",
        "entropy_results = {}\n",
        "\n",
        "for question_id, data in clustered_data.items():\n",
        "    clusters = data[\"clusters\"]\n",
        "\n",
        "    # Step 1: Compute cluster probabilities\n",
        "    probabilities = compute_cluster_uniform_probabilities(clusters)\n",
        "\n",
        "    # Step 2: Compute semantic entropy\n",
        "    entropy = compute_semantic_entropy(probabilities)\n",
        "\n",
        "    # Store results\n",
        "    entropy_results[question_id] = {\n",
        "        \"question\": data[\"question\"],\n",
        "        \"semantic_entropy\": entropy,\n",
        "        \"num_clusters\": len(clusters),\n",
        "        \"probabilities\": probabilities\n",
        "    }\n",
        "\n",
        "# Save the results\n",
        "output_file = \"semantic_entropy_Llama3.1-mlqa_results.json\"\n",
        "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(entropy_results, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "files.download(output_file)\n",
        "print(f\"Semantic entropy results saved to {output_file}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rO9ZNtBJV0IR",
        "outputId": "7b486e86-725c-429f-efb3-a0ade35bb4eb"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Semantic entropy results saved to semantic_entropy_Llama3.1-mlqa_results.json\n"
          ]
        }
      ]
    }
  ]
}