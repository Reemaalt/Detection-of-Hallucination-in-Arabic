{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Reemaalt/Detection-of-Hallucination-in-Arabic/blob/main/Semantic_Entropy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S-CbnTf8Ivac",
        "outputId": "0ae5c3a4-cd91-4296-98cb-3355fe55dcf1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    To log in, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Enter your token (input will not be visible): \n",
            "Add token as git credential? (Y/n) n\n",
            "Token is valid (permission: fineGrained).\n",
            "The token `week1 test` has been saved to /root/.cache/huggingface/stored_tokens\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful.\n",
            "The current active token is: `week1 test`\n"
          ]
        }
      ],
      "source": [
        "!huggingface-cli login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rcBbLGoZCZUP",
        "outputId": "35982f2c-ff76-4caf-d562-3fd388fb4fba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "# Semantic Entropy code Based on the original implementation by the new githup\n",
        "import json\n",
        "import os\n",
        "import pickle\n",
        "import random\n",
        "from functools import lru_cache\n",
        "from tqdm import tqdm\n",
        "import zipfile\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from transformers import ElectraTokenizerFast, ElectraForSequenceClassification, AutoModelForCausalLM, AutoTokenizer\n",
        "from google.colab import files\n",
        "from tqdm.notebook import tqdm\n",
        "import os\n",
        "import json\n",
        "from google.colab import files\n",
        "# Set up device and logging\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {DEVICE}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HDVS2hTwDcwg"
      },
      "source": [
        "#EntailmentModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IOm4n6WiGCbQ",
        "outputId": "0a3c6881-d228-4aa1-8355-8c658b18bb79"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Extraction complete.\n"
          ]
        }
      ],
      "source": [
        "#ues our trained fine-tunied model\n",
        "#get the finetuned model from drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "zip_path = \"/content/drive/My Drive/araelectra-nli-finetuned.zip\"  # Adjust the path if needed\n",
        "extract_path = \"/content/araelectra\"\n",
        "\n",
        "# Extract the zip file\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "print(\"Extraction complete.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "BtaTMvXqCvS3"
      },
      "outputs": [],
      "source": [
        "# Modify the ArabicEntailmentModel class to handle dictionary arguments properly\n",
        "class ArabicEntailmentModel:\n",
        "    \"\"\"\n",
        "    Arabic entailment checker using our AraELECTRA model.\n",
        "    \"\"\"\n",
        "    def __init__(self, model_path=\"/content/araelectra/araelectra-nli-finetuned\"):\n",
        "        \"\"\"\n",
        "        Parameters:\n",
        "        - model_path: Path to the fine-tuned AraELECTRA model\n",
        "        \"\"\"\n",
        "\n",
        "        print(\"Loading AraELECTRA model for Arabic entailment checking...\")\n",
        "        self.tokenizer = ElectraTokenizerFast.from_pretrained(model_path)\n",
        "        self.model = ElectraForSequenceClassification.from_pretrained(model_path)\n",
        "        self.model = self.model.to(DEVICE)\n",
        "        self.model.eval()  # Set to evaluation mode\n",
        "        self.cache = {}\n",
        "        # Add a file-based persistent cache\n",
        "        self.cache_file = \"entailment_cache.pkl\"\n",
        "        self._load_cache()\n",
        "        print(\"AraELECTRA model loaded successfully\")\n",
        "\n",
        "    def _load_cache(self):\n",
        "        try:\n",
        "            if os.path.exists(self.cache_file):\n",
        "                with open(self.cache_file, 'rb') as f:\n",
        "                    loaded_cache = pickle.load(f)\n",
        "                    self.cache.update(loaded_cache)\n",
        "                print(f\"Loaded {len(self.cache)} cached entailment results\")\n",
        "        except Exception as e:\n",
        "            print(f\"Cache loading failed: {e}\")\n",
        "\n",
        "    def _save_cache(self):\n",
        "        try:\n",
        "            with open(self.cache_file, 'wb') as f:\n",
        "                pickle.dump(self.cache, f)\n",
        "        except Exception as e:\n",
        "            print(f\"Cache saving failed: {e}\")\n",
        "\n",
        "    def check_implication(self, text1, text2, *args, **kwargs):\n",
        "        \"\"\"\n",
        "        Check entailment between text1 and text2.\n",
        "\n",
        "        Parameters:\n",
        "        - text1: First text (premise)\n",
        "        - text2: Second text (hypothesis)\n",
        "\n",
        "        Returns:\n",
        "        - 0 for contradiction, 1 for neutral, 2 for entailment (matching semantic entropy code)\n",
        "        \"\"\"\n",
        "        # Create a cache key from the text inputs\n",
        "        cache_key = (text1, text2)\n",
        "        # Check if result is already in cache\n",
        "        if cache_key in self.cache:\n",
        "            return self.cache[cache_key]\n",
        "\n",
        "        # Prepare input\n",
        "        inputs = self.tokenizer(\n",
        "            text1,\n",
        "            text2,\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=128,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        # Move inputs to device\n",
        "        inputs = {k: v.to(DEVICE) for k, v in inputs.items()}\n",
        "\n",
        "        # Get prediction\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(**inputs)\n",
        "            logits = outputs.logits\n",
        "            probs = F.softmax(logits, dim=1)\n",
        "            predicted_class = torch.argmax(probs, dim=1).item()\n",
        "\n",
        "        # Map prediction to match semantic entropy code:\n",
        "        # 0: contradiction, 1: neutral, 2: entailment\n",
        "        result_map = {0: 2, 1: 1, 2: 0}  # Convert AraELECTRA output to our expected format {0: \"entailment\", 1: \"neutral\", 2: \"contradiction\"}\n",
        "        result = result_map.get(predicted_class)\n",
        "\n",
        "        # Save cache periodically (e.g., every 100 new entries)\n",
        "        if len(self.cache) % 100 == 0:\n",
        "            self._save_cache()\n",
        "\n",
        "        return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ulsao1Uw5jKm"
      },
      "source": [
        "# Clustering\n",
        "\n",
        "#also from their github got this\n",
        "\n",
        "```\n",
        "def get_semantic_ids(strings_list, model, strict_entailment=False, example=None):\n",
        "    \"\"\"Group list of predictions into semantic meaning.\"\"\"\n",
        "\n",
        "    def are_equivalent(text1, text2):\n",
        "\n",
        "        implication_1 = model.check_implication(text1, text2, example=example)\n",
        "        implication_2 = model.check_implication(text2, text1, example=example)  # pylint: disable=arguments-out-of-order\n",
        "        assert (implication_1 in [0, 1, 2]) and (implication_2 in [0, 1, 2])\n",
        "\n",
        "        if strict_entailment:\n",
        "            semantically_equivalent = (implication_1 == 2) and (implication_2 == 2)\n",
        "\n",
        "        else:\n",
        "            implications = [implication_1, implication_2]\n",
        "            # Check if none of the implications are 0 (contradiction) and not both of them are neutral.\n",
        "            semantically_equivalent = (0 not in implications) and ([1, 1] != implications)\n",
        "\n",
        "        return semantically_equivalent\n",
        "\n",
        "    # Initialise all ids with -1.\n",
        "    semantic_set_ids = [-1] * len(strings_list)\n",
        "    # Keep track of current id.\n",
        "    next_id = 0\n",
        "    for i, string1 in enumerate(strings_list):\n",
        "        # Check if string1 already has an id assigned.\n",
        "        if semantic_set_ids[i] == -1:\n",
        "            # If string1 has not been assigned an id, assign it next_id.\n",
        "            semantic_set_ids[i] = next_id\n",
        "            for j in range(i+1, len(strings_list)):\n",
        "                # Search through all remaining strings. If they are equivalent to string1, assign them the same id.\n",
        "                if are_equivalent(string1, strings_list[j]):\n",
        "                    semantic_set_ids[j] = next_id\n",
        "            next_id += 1\n",
        "\n",
        "    assert -1 not in semantic_set_ids\n",
        "\n",
        "    return semantic_set_ids\n",
        "\n",
        "\n",
        "def logsumexp_by_id(semantic_ids, log_likelihoods, agg='sum_normalized'):\n",
        "    \"\"\"Sum probabilities with the same semantic id.\n",
        "\n",
        "    Log-Sum-Exp because input and output probabilities in log space.\n",
        "    \"\"\"\n",
        "    unique_ids = sorted(list(set(semantic_ids)))\n",
        "    assert unique_ids == list(range(len(unique_ids)))\n",
        "    log_likelihood_per_semantic_id = []\n",
        "\n",
        "    for uid in unique_ids:\n",
        "        # Find positions in `semantic_ids` which belong to the active `uid`.\n",
        "        id_indices = [pos for pos, x in enumerate(semantic_ids) if x == uid]\n",
        "        # Gather log likelihoods at these indices.\n",
        "        id_log_likelihoods = [log_likelihoods[i] for i in id_indices]\n",
        "        if agg == 'sum_normalized':\n",
        "            # log_lik_norm = id_log_likelihoods - np.prod(log_likelihoods)\n",
        "            log_lik_norm = id_log_likelihoods - np.log(np.sum(np.exp(log_likelihoods)))\n",
        "            logsumexp_value = np.log(np.sum(np.exp(log_lik_norm)))\n",
        "        else:\n",
        "            raise ValueError\n",
        "        log_likelihood_per_semantic_id.append(logsumexp_value)\n",
        "\n",
        "    return log_likelihood_per_semantic_id\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "NW3ag2a4LHhN"
      },
      "outputs": [],
      "source": [
        "def get_semantic_ids(strings_list, model, strict_entailment=False, example=None):\n",
        "\n",
        "    # Group list of predictions into semantic meaning\n",
        "\n",
        "    @lru_cache(maxsize=1024)\n",
        "    def are_equivalent(text1, text2):\n",
        "        # Check if text1 entails text2\n",
        "        implication_1 = model.check_implication(text1, text2, example=example)\n",
        "        # Check if text2 entails text1\n",
        "        implication_2 = model.check_implication(text2, text1, example=example)\n",
        "        assert (implication_1 in [0, 1, 2]) and (implication_2 in [0, 1, 2])\n",
        "\n",
        "        if strict_entailment:\n",
        "            # Both must indicate entailment (2) for semantic equivalence\n",
        "            semantically_equivalent = (implication_1 == 2) and (implication_2 == 2)\n",
        "        else:\n",
        "            implications = [implication_1, implication_2]\n",
        "            # Check if none of the implications are 0 (contradiction) and not both of them are neutral.)\n",
        "            semantically_equivalent = (0 not in implications) and ([1, 1] != implications)\n",
        "\n",
        "        return semantically_equivalent\n",
        "\n",
        "    # Initialize all ids with -1\n",
        "    semantic_set_ids = [-1] * len(strings_list)\n",
        "    # Keep track of current id\n",
        "    next_id = 0\n",
        "\n",
        "    for i, string1 in enumerate(strings_list):\n",
        "        # Check if string1 already has an id assigned\n",
        "        if semantic_set_ids[i] == -1:\n",
        "            # If string1 has not been assigned an id, assign it next_id\n",
        "            semantic_set_ids[i] = next_id\n",
        "            for j in range(i+1, len(strings_list)):\n",
        "                # Search through all remaining strings. If they are equivalent to string1, assign them the same id.\n",
        "                if semantic_set_ids[j] == -1 and are_equivalent(string1, strings_list[j]):\n",
        "                    semantic_set_ids[j] = next_id\n",
        "            next_id += 1\n",
        "\n",
        "    assert -1 not in semantic_set_ids\n",
        "\n",
        "    return semantic_set_ids\n",
        "\n",
        "def logsumexp_by_id(semantic_ids, log_likelihoods, agg='sum_normalized'):\n",
        "    \"\"\"\n",
        "    Sum probabilities with the same semantic ID.\n",
        "    Log-Sum-Exp because input and output probabilities in log space.\n",
        "    \"\"\"\n",
        "    unique_ids = sorted(list(set(semantic_ids)))\n",
        "    assert unique_ids == list(range(len(unique_ids)))\n",
        "    log_likelihood_per_semantic_id = []\n",
        "\n",
        "    for uid in unique_ids:\n",
        "        # Find positions in `semantic_ids` which belong to the active `uid`\n",
        "        id_indices = [pos for pos, x in enumerate(semantic_ids) if x == uid]\n",
        "        # Gather log likelihoods at these indices\n",
        "        id_log_likelihoods = [log_likelihoods[i] for i in id_indices]\n",
        "\n",
        "        if agg == 'sum_normalized':\n",
        "            # Normalize by subtracting the log sum exp of all log likelihoods\n",
        "           # log_lik_norm = id_log_likelihoods - np.prod(log_likelihoods)\n",
        "            log_lik_norm = id_log_likelihoods - np.log(np.sum(np.exp(log_likelihoods)))\n",
        "            logsumexp_value = np.log(np.sum(np.exp(log_lik_norm)))\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown aggregation method: {agg}\")\n",
        "\n",
        "        log_likelihood_per_semantic_id.append(logsumexp_value)\n",
        "\n",
        "    return log_likelihood_per_semantic_id"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2wHskqM9lkQ"
      },
      "source": [
        "#entropy\n",
        "\n",
        "#from their github i got this code\n",
        "\n",
        "```\n",
        "\n",
        "def predictive_entropy(log_probs):\n",
        "    \"\"\"Compute MC estimate of entropy.\n",
        "\n",
        "    `E[-log p(x)] ~= -1/N sum_i log p(x_i)`, i.e. the average token likelihood.\n",
        "    \"\"\"\n",
        "\n",
        "    entropy = -np.sum(log_probs) / len(log_probs)\n",
        "\n",
        "    return entropy\n",
        "\n",
        "\n",
        "def predictive_entropy_rao(log_probs):\n",
        "    entropy = -np.sum(np.exp(log_probs) * log_probs)\n",
        "    return entropy\n",
        "\n",
        "\n",
        "def cluster_assignment_entropy(semantic_ids):\n",
        "    \"\"\"Estimate semantic uncertainty from how often different clusters get assigned.\n",
        "\n",
        "    We estimate the categorical distribution over cluster assignments from the\n",
        "    semantic ids. The uncertainty is then given by the entropy of that\n",
        "    distribution. This estimate does not use token likelihoods, it relies soley\n",
        "    on the cluster assignments. If probability mass is spread of between many\n",
        "    clusters, entropy is larger. If probability mass is concentrated on a few\n",
        "    clusters, entropy is small.\n",
        "\n",
        "    Input:\n",
        "        semantic_ids: List of semantic ids, e.g. [0, 1, 2, 1].\n",
        "    Output:\n",
        "        cluster_entropy: Entropy, e.g. (-p log p).sum() for p = [1/4, 2/4, 1/4].\n",
        "    \"\"\"\n",
        "\n",
        "    n_generations = len(semantic_ids)\n",
        "    counts = np.bincount(semantic_ids)\n",
        "    probabilities = counts/n_generations\n",
        "    assert np.isclose(probabilities.sum(), 1)\n",
        "    entropy = - (probabilities * np.log(probabilities)).sum()\n",
        "    return entropy\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Euc93oDFGKCa"
      },
      "outputs": [],
      "source": [
        "def predictive_entropy_rao(log_probs):\n",
        "    \"\"\"\n",
        "    Compute entropy from log probabilities.\n",
        "\n",
        "    Parameters:\n",
        "    - log_probs: Log probabilities\n",
        "\n",
        "    Returns:\n",
        "    - Entropy value\n",
        "    \"\"\"\n",
        "    entropy = -np.sum(np.exp(log_probs) * log_probs)\n",
        "    return entropy\n",
        "\n",
        "def predictive_entropy(log_probs):\n",
        "    \"\"\"Compute MC estimate of entropy.\n",
        "\n",
        "    `E[-log p(x)] ~= -1/N sum_i log p(x_i)`, i.e. the average token likelihood.\n",
        "    \"\"\"\n",
        "\n",
        "    entropy = -np.sum(log_probs) / len(log_probs)\n",
        "\n",
        "    return entropy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnKq6Ixs9iSr"
      },
      "source": [
        "# call function\n",
        "The actual full computation in the original repo happens in compute_uncertainty_measures.py:\n",
        " This  code is where they call the functions to compute the entropy measures\n",
        "\n",
        "```\n",
        "\n",
        "if args.compute_predictive_entropy:\n",
        "    # Token log likelihoods. Shape = (n_sample, n_tokens)\n",
        "    if not args.use_all_generations:\n",
        "        log_liks = [r[1] for r in full_responses[:args.use_num_generations]]\n",
        "    else:\n",
        "        log_liks = [r[1] for r in full_responses]\n",
        "\n",
        "    for i in log_liks:\n",
        "        assert i\n",
        "\n",
        "    if args.compute_context_entails_response:\n",
        "        # Compute context entails answer baseline.\n",
        "        entropies['context_entails_response'].append(context_entails_response(\n",
        "            context, responses, entailment_model))\n",
        "\n",
        "    if args.condition_on_question and args.entailment_model == 'deberta':\n",
        "        responses = [f'{question} {r}' for r in responses]\n",
        "\n",
        "    # Compute semantic ids.\n",
        "    semantic_ids = get_semantic_ids(\n",
        "        responses, model=entailment_model,\n",
        "        strict_entailment=args.strict_entailment, example=example)\n",
        "\n",
        "    result_dict['semantic_ids'].append(semantic_ids)\n",
        "\n",
        "    # Compute entropy from frequencies of cluster assignments.\n",
        "    entropies['cluster_assignment_entropy'].append(cluster_assignment_entropy(semantic_ids))\n",
        "\n",
        "    # Length normalization of generation probabilities.\n",
        "    log_liks_agg = [np.mean(log_lik) for log_lik in log_liks]\n",
        "\n",
        "    # Compute naive entropy.\n",
        "    entropies['regular_entropy'].append(predictive_entropy(log_liks_agg))\n",
        "\n",
        "    # Compute semantic entropy.\n",
        "    log_likelihood_per_semantic_id = logsumexp_by_id(semantic_ids, log_liks_agg, agg='sum_normalized')\n",
        "    pe = predictive_entropy_rao(log_likelihood_per_semantic_id)\n",
        "    entropies['semantic_entropy'].append(pe)\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "_uy0aCxkIL_R"
      },
      "outputs": [],
      "source": [
        "def experiment_semantic_entropy(question, original_answer ,llm_model, llm_tokenizer, entailment_model, num_samples):\n",
        "    \"\"\"\n",
        "    Compute semantic entropy for a given question.\n",
        "    Returns:\n",
        "        Dictionary containing results of the experiment\n",
        "    \"\"\"\n",
        "    # Step 1: Generate multiple answers with their token log likelihoods\n",
        "    #print(f\"Generating {num_samples} answers for question: {question}\")\n",
        "    results = generate_answer(question, num_samples, llm_model, llm_tokenizer)\n",
        "\n",
        "    # Step 2: Extract answers and their log likelihoods\n",
        "    answers = [result['text'] for result in results]\n",
        "    # length normalization of the log probabilities\n",
        "    log_likelihoods = [np.mean(result['token_log_probs']) for result in results]\n",
        "\n",
        "   # print(f\"Generated {len(answers)} answers\")\n",
        "\n",
        "    # Step 3: Create an example dictionary for entailment checking\n",
        "    example = {'question': question}\n",
        "\n",
        "    # Step 4: Compute semantic clusters\n",
        "   # print(\"Computing semantic clusters...\")\n",
        "    semantic_ids = get_semantic_ids(answers, entailment_model, strict_entailment=False, example=example)\n",
        "    unique_clusters = len(set(semantic_ids))\n",
        "   # print(f\"Found {unique_clusters} semantic clusters\")\n",
        "\n",
        "    # Step 5: Calculate entropy measures\n",
        "    # naive_entropy calculation (based on log likelihoods only)\n",
        "    naive_entropy = predictive_entropy(log_likelihoods)\n",
        "\n",
        "    # Semantic entropy calculation (based on semantic clusters and log likelihoods)\n",
        "    log_likelihood_per_semantic_id = logsumexp_by_id(semantic_ids, log_likelihoods, agg='sum_normalized')\n",
        "    semantic_entropy = predictive_entropy_rao(log_likelihood_per_semantic_id)\n",
        "\n",
        "    # Step 6: Print results\n",
        "    print(f\"\\nEntropy Analysis for: '{question}'\")\n",
        "    print(f\"Generated {len(answers)} answers in {unique_clusters} semantic clusters\")\n",
        "    #print the entropy values with 4 decimal places\n",
        "    print(f\" naive Entropy: {naive_entropy:.4f}\")\n",
        "    print(f\"Semantic Entropy: {semantic_entropy:.4f}\")\n",
        "\n",
        "\n",
        "    # Step 7: Display cluster information\n",
        "    #print(\"\\nSemantic Clusters:\")\n",
        "    unique_clusters_list = sorted(list(set(semantic_ids)))\n",
        "    for cluster_id in unique_clusters_list:\n",
        "        cluster_items = [answers[i] for i, sid in enumerate(semantic_ids) if sid == cluster_id]\n",
        "        count = len(cluster_items)\n",
        "        '''\n",
        "        print(f\"\\nCluster {cluster_id} ({count} items):\")\n",
        "        for item in cluster_items:\n",
        "            print(f\"  - {item}\")\n",
        "        '''\n",
        "    # Format answers in the  structure\n",
        "    formatted_answers = []\n",
        "    for cluster_id in unique_clusters_list:\n",
        "        cluster_answers = [answers[i] for i, sid in enumerate(semantic_ids) if sid == cluster_id]\n",
        "        formatted_answers.append(cluster_answers)\n",
        "\n",
        "    # Return  results\n",
        "    # number like 2.220446049250313e-16 is extremely close to zero (basically floating-point noise).\n",
        "    # so rounds the number to 4 decimal places\n",
        "  # Return results with details and formatted answers\n",
        "    return {\n",
        "        'question': question,\n",
        "        'original_answer': original_answer,  # Include the original answer from the dataset\n",
        "        'answers': formatted_answers,  # Answers formatted\n",
        "        'log_likelihoods': log_likelihoods,\n",
        "        'semantic_ids': semantic_ids,\n",
        "        'naive_entropy': round(naive_entropy, 4),\n",
        "        'semantic_entropy': round(semantic_entropy, 4),\n",
        "        'num_clusters': unique_clusters,\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0EoVG07fDNN"
      },
      "source": [
        "#get likelihood\n",
        "\n",
        "#from their code\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "#at this function\n",
        "def predict(self, input_data, temperature, return_full=False):\n",
        "\n",
        "\n",
        "        # Get log_likelihoods.\n",
        "        # outputs.scores are the logits for the generated token.\n",
        "        # outputs.scores is a tuple of len = n_generated_tokens.\n",
        "        # Each entry is shape (bs, vocabulary size).\n",
        "        # outputs.sequences is the sequence of all tokens: input and generated.\n",
        "        transition_scores = self.model.compute_transition_scores(\n",
        "            outputs.sequences, outputs.scores, normalize_logits=True)\n",
        "        # Transition_scores[0] only contains the scores for the first generated tokens.\n",
        "\n",
        "        log_likelihoods = [score.item() for score in transition_scores[0]]\n",
        "        if len(log_likelihoods) == 1:\n",
        "            logging.warning('Taking first and only generation for log likelihood!')\n",
        "            log_likelihoods = log_likelihoods\n",
        "        else:\n",
        "            log_likelihoods = log_likelihoods[:n_generated]\n",
        "\n",
        "        if len(log_likelihoods) == self.max_new_tokens:\n",
        "            logging.warning('Generation interrupted by max_token limit.')\n",
        "\n",
        "        if len(log_likelihoods) == 0:\n",
        "            raise ValueError\n",
        "\n",
        "        return sliced_answer, log_likelihoods, last_token_embedding\n",
        "\n",
        "\n",
        "all function if needed\n",
        "f predict(self, input_data, temperature, return_full=False):\n",
        "\n",
        "        # Implement prediction.\n",
        "        inputs = self.tokenizer(input_data, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "        if 'llama' in self.model_name.lower() or 'falcon' in self.model_name or 'mistral' in self.model_name.lower():\n",
        "            if 'token_type_ids' in inputs:  # Some HF models have changed.\n",
        "                del inputs['token_type_ids']\n",
        "            pad_token_id = self.tokenizer.eos_token_id\n",
        "        else:\n",
        "            pad_token_id = None\n",
        "\n",
        "        if self.stop_sequences is not None:\n",
        "            stopping_criteria = StoppingCriteriaList([StoppingCriteriaSub(\n",
        "                stops=self.stop_sequences,\n",
        "                initial_length=len(inputs['input_ids'][0]),\n",
        "                tokenizer=self.tokenizer)])\n",
        "        else:\n",
        "            stopping_criteria = None\n",
        "\n",
        "        logging.debug('temperature: %f', temperature)\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=self.max_new_tokens,\n",
        "                return_dict_in_generate=True,\n",
        "                output_scores=True,\n",
        "                output_hidden_states=True,\n",
        "                temperature=temperature,\n",
        "                do_sample=True,\n",
        "                stopping_criteria=stopping_criteria,\n",
        "                pad_token_id=pad_token_id,\n",
        "            )\n",
        "\n",
        "        if len(outputs.sequences[0]) > self.token_limit:\n",
        "            raise ValueError(\n",
        "                'Generation exceeding token limit %d > %d',\n",
        "                len(outputs.sequences[0]), self.token_limit)\n",
        "\n",
        "        full_answer = self.tokenizer.decode(\n",
        "            outputs.sequences[0], skip_special_tokens=True)\n",
        "\n",
        "        if return_full:\n",
        "            return full_answer\n",
        "\n",
        "        # For some models, we need to remove the input_data from the answer.\n",
        "        if full_answer.startswith(input_data):\n",
        "            input_data_offset = len(input_data)\n",
        "        else:\n",
        "            raise ValueError('Have not tested this in a while.')\n",
        "\n",
        "        # Remove input from answer.\n",
        "        answer = full_answer[input_data_offset:]\n",
        "\n",
        "        # Remove stop_words from answer.\n",
        "        stop_at = len(answer)\n",
        "        sliced_answer = answer\n",
        "        if self.stop_sequences is not None:\n",
        "            for stop in self.stop_sequences:\n",
        "                if answer.endswith(stop):\n",
        "                    stop_at = len(answer) - len(stop)\n",
        "                    sliced_answer = answer[:stop_at]\n",
        "                    break\n",
        "            if not all([stop not in sliced_answer for stop in self.stop_sequences]):\n",
        "                error_msg = 'Error: Stop words not removed successfully!'\n",
        "                error_msg += f'Answer: >{answer}< '\n",
        "                error_msg += f'Sliced Answer: >{sliced_answer}<'\n",
        "                if 'falcon' not in self.model_name.lower():\n",
        "                    raise ValueError(error_msg)\n",
        "                else:\n",
        "                    logging.error(error_msg)\n",
        "\n",
        "        # Remove whitespaces from answer (in particular from beginning.)\n",
        "        sliced_answer = sliced_answer.strip()\n",
        "\n",
        "        # Get the number of tokens until the stop word comes up.\n",
        "        # Note: Indexing with `stop_at` already excludes the stop_token.\n",
        "        # Note: It's important we do this with full answer, since there might be\n",
        "        # non-trivial interactions between the input_data and generated part\n",
        "        # in tokenization (particularly around whitespaces.)\n",
        "        token_stop_index = self.tokenizer(full_answer[:input_data_offset + stop_at], return_tensors=\"pt\")['input_ids'].shape[1]\n",
        "        n_input_token = len(inputs['input_ids'][0])\n",
        "        n_generated = token_stop_index - n_input_token\n",
        "\n",
        "        if n_generated == 0:\n",
        "            logging.warning('Only stop_words were generated. For likelihoods and embeddings, taking stop word instead.')\n",
        "            n_generated = 1\n",
        "\n",
        "        # Get the last hidden state (last layer) and the last token's embedding of the answer.\n",
        "        # Note: We do not want this to be the stop token.\n",
        "\n",
        "        # outputs.hidden_state is a tuple of len = n_generated_tokens.\n",
        "        # The first hidden state is for the input tokens and is of shape\n",
        "        #     (n_layers) x (batch_size, input_size, hidden_size).\n",
        "        # (Note this includes the first generated token!)\n",
        "        # The remaining hidden states are for the remaining generated tokens and is of shape\n",
        "        #    (n_layers) x (batch_size, 1, hidden_size).\n",
        "\n",
        "        # Note: The output embeddings have the shape (batch_size, generated_length, hidden_size).\n",
        "        # We do not get embeddings for input_data! We thus subtract the n_tokens_in_input from\n",
        "        # token_stop_index to arrive at the right output.\n",
        "\n",
        "        if 'decoder_hidden_states' in outputs.keys():\n",
        "            hidden = outputs.decoder_hidden_states\n",
        "        else:\n",
        "            hidden = outputs.hidden_states\n",
        "\n",
        "        if len(hidden) == 1:\n",
        "            logging.warning(\n",
        "                'Taking first and only generation for hidden! '\n",
        "                'n_generated: %d, n_input_token: %d, token_stop_index %d, '\n",
        "                'last_token: %s, generation was: %s',\n",
        "                n_generated, n_input_token, token_stop_index,\n",
        "                self.tokenizer.decode(outputs['sequences'][0][-1]),\n",
        "                full_answer,\n",
        "                )\n",
        "            last_input = hidden[0]\n",
        "        elif ((n_generated - 1) >= len(hidden)):\n",
        "            # If access idx is larger/equal.\n",
        "            logging.error(\n",
        "                'Taking last state because n_generated is too large'\n",
        "                'n_generated: %d, n_input_token: %d, token_stop_index %d, '\n",
        "                'last_token: %s, generation was: %s, slice_answer: %s',\n",
        "                n_generated, n_input_token, token_stop_index,\n",
        "                self.tokenizer.decode(outputs['sequences'][0][-1]),\n",
        "                full_answer, sliced_answer\n",
        "                )\n",
        "            last_input = hidden[-1]\n",
        "        else:\n",
        "            last_input = hidden[n_generated - 1]\n",
        "\n",
        "        # Then access last layer for input\n",
        "        last_layer = last_input[-1]\n",
        "        # Then access last token in input.\n",
        "        last_token_embedding = last_layer[:, -1, :].cpu()\n",
        "\n",
        "        # Get log_likelihoods.\n",
        "        # outputs.scores are the logits for the generated token.\n",
        "        # outputs.scores is a tuple of len = n_generated_tokens.\n",
        "        # Each entry is shape (bs, vocabulary size).\n",
        "        # outputs.sequences is the sequence of all tokens: input and generated.\n",
        "        transition_scores = self.model.compute_transition_scores(\n",
        "            outputs.sequences, outputs.scores, normalize_logits=True)\n",
        "        # Transition_scores[0] only contains the scores for the first generated tokens.\n",
        "\n",
        "        log_likelihoods = [score.item() for score in transition_scores[0]]\n",
        "        if len(log_likelihoods) == 1:\n",
        "            logging.warning('Taking first and only generation for log likelihood!')\n",
        "            log_likelihoods = log_likelihoods\n",
        "        else:\n",
        "            log_likelihoods = log_likelihoods[:n_generated]\n",
        "\n",
        "        if len(log_likelihoods) == self.max_new_tokens:\n",
        "            logging.warning('Generation interrupted by max_token limit.')\n",
        "\n",
        "        if len(log_likelihoods) == 0:\n",
        "            raise ValueError\n",
        "\n",
        "        return sliced_answer, log_likelihoods, last_token_embedding\n",
        "\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFYlG7k_gCIS"
      },
      "source": [
        "##log probabilities vs negative log likelihoods:\n",
        "\n",
        "- Log probabilities are the logarithm of the probability: log(p)\n",
        "- Negative log likelihoods are the negative logarithm of the probability: -log(p)\n",
        "\n",
        "The original code is working with log probabilities, not negative log likelihoods. We see normalize_logits=True in compute_transition_scores, it means the model is returning log probabilities.\n",
        "\n",
        "\n",
        "**The original code is using predictive_entropy_rao, which expects log probabilities as input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JXUkp0AdvHBb"
      },
      "outputs": [],
      "source": [
        "def generate_answer(question, num_samples, model, tokenizer):\n",
        "    \"\"\"\n",
        "    Generate multiple answers to a question using LLM with direct token log likelihoods.\n",
        "    \"\"\"\n",
        "    # Create the prompt with the question\n",
        "    prompt = f\"أجب على السؤال التالي بجملة واحدة فقط موجزة ولكن كاملة باللغة العربية\\nQuestion: {question}\\nAnswer:\"\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for _ in range(num_samples):\n",
        "        # Tokenize the prompt\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "        prompt_length = inputs.input_ids.shape[1]  # Number of tokens in the prompt\n",
        "\n",
        "        # Generate with return_dict_in_generate=True and output_scores=True to get scores\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                inputs.input_ids,\n",
        "                max_new_tokens=100,\n",
        "                do_sample=True,\n",
        "                temperature=0.5,\n",
        "                return_dict_in_generate=True,\n",
        "                output_scores=True,\n",
        "            )\n",
        "\n",
        "        # Calculate token log probabilities using compute_transition_scores\n",
        "        # normalize_logits=True ensures we get log probabilities\n",
        "        transition_scores = model.compute_transition_scores(\n",
        "            outputs.sequences,\n",
        "            outputs.scores,\n",
        "            normalize_logits=True\n",
        "        )\n",
        "\n",
        "        # Extract log likelihoods like they did exactly, but no handeling of cases\n",
        "        log_likelihoods = [score.item() for score in transition_scores[0]]\n",
        "\n",
        "\n",
        "        # Decode the generated text\n",
        "        generated_text = tokenizer.decode(outputs.sequences[0], skip_special_tokens=True)\n",
        "        answer = generated_text.split(\"Answer:\")[-1].strip()\n",
        "\n",
        "        # Clean the output\n",
        "        strings_to_filter_on = ['.', '\\n', 'Q:', 'A:', 'question:', 'answer:', 'Question:', 'Answer:',\n",
        "                               'Questions:', 'questions:', 'QUESTION:', 'ANSWER:']\n",
        "        for string in strings_to_filter_on:\n",
        "            if string in answer:\n",
        "                answer = answer.split(string)[0].strip()\n",
        "\n",
        "        results.append({\n",
        "            'text': answer,\n",
        "            'token_log_probs': log_likelihoods,  # Store raw log probabilities\n",
        "\n",
        "        })\n",
        "\n",
        "    return results # Return results AFTER the loop completes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKCz4R7jnK7D"
      },
      "source": [
        "# Main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a8ag496J_opE"
      },
      "outputs": [],
      "source": [
        "def load_qa_dataset(dataset_name, file_path=None):\n",
        "    data = []\n",
        "\n",
        "    try:\n",
        "        if dataset_name == 'arabicaqa' and file_path:\n",
        "            if os.path.exists(file_path):  # Use the loaded file\n",
        "                print(\"Using ArabicaQA\")\n",
        "                with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                    custom_data = json.load(f)\n",
        "                    for idx, item in enumerate(custom_data):\n",
        "                        data.append({\n",
        "                            \"question_id\": idx,\n",
        "                            \"Question\": item[\"question\"],\n",
        "                            \"Answer\": item[\"answer\"]\n",
        "                        })\n",
        "            else:\n",
        "                raise FileNotFoundError(f\"ArabicaQA file not found at {file_path}\")\n",
        "\n",
        "        elif dataset_name == 'xor_tydiqa' and file_path:\n",
        "            if os.path.exists(file_path):  # Use the loaded file\n",
        "                print(\"Using XOR-TyDiQA\")\n",
        "                print(\"Filtering Arabic QA pairs from XOR-TyDi...\")\n",
        "\n",
        "                # Load the jsonl dataset\n",
        "                with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                    custom_data = [json.loads(line) for line in f]  # Handling jsonl\n",
        "\n",
        "                # Filter Arabic samples (\"lang\" = \"ar\")\n",
        "                arabic_data = []\n",
        "                for idx, item in enumerate(custom_data):\n",
        "                    if item[\"lang\"] == \"ar\":  # Arabic language code\n",
        "                        arabic_data.append({\n",
        "                            \"question_id\": item[\"id\"],\n",
        "                            \"Question\": item[\"question\"],\n",
        "                            \"Answer\": item[\"answers\"][0]  # First answer in list\n",
        "                        })\n",
        "\n",
        "                data.extend(arabic_data)\n",
        "            else:\n",
        "                raise FileNotFoundError(f\"XOR-TyDiQA file not found at {file_path}\")\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported dataset: {dataset_name}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading {dataset_name}: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "referenced_widgets": [
            "e9b0f9f785e3443c83714f603b723425",
            "4aa48c641e3a4249b9a2a04d917ff5b8"
          ]
        },
        "id": "5B6fdiUCITVs",
        "outputId": "4c2a6c20-f16e-46b3-bea5-2451cf588168"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading language model...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e9b0f9f785e3443c83714f603b723425",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded ALLaM model successfully\n",
            "Loading entailment model...\n",
            "Loading AraELECTRA model for Arabic entailment checking...\n",
            "Loaded 0 cached entailment results\n",
            "AraELECTRA model loaded successfully\n",
            "Entailment model loaded successfully\n",
            "Loading dataset...\n",
            "Using ArabicaQA\n",
            "Loaded 12592 questions from arabicaqa\n",
            "\n",
            "Starting semantic entropy experiments...\n",
            "✓ No previous checkpoint found. Starting fresh.\n",
            "Starting from question 1\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4aa48c641e3a4249b9a2a04d917ff5b8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Processing questions:   0%|          | 0/12592 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[1/12592] Processing question\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Entropy Analysis for: 'من الذي يترأس المرصد الحضري لمدينة الرياض؟'\n",
            "Generated 10 answers in 4 semantic clusters\n",
            " naive Entropy: 0.3710\n",
            "Semantic Entropy: 1.1271\n",
            "\n",
            "[2/12592] Processing question\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Entropy Analysis for: 'ما هي الإنجازات البارزة التي حققها المرصد الحضري بمدينة الرياض؟'\n",
            "Generated 10 answers in 6 semantic clusters\n",
            " naive Entropy: 0.7847\n",
            "Semantic Entropy: 1.6329\n",
            "\n",
            "[3/12592] Processing question\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Entropy Analysis for: 'ما هو المرصد الحضري لمدينة الرياض؟'\n",
            "Generated 10 answers in 2 semantic clusters\n",
            " naive Entropy: 0.5208\n",
            "Semantic Entropy: 0.3148\n",
            "\n",
            "[4/12592] Processing question\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Function to safely load existing partial results\n",
        "def load_partial_results(partial_file_path):\n",
        "    if os.path.exists(partial_file_path):\n",
        "        with open(partial_file_path, 'r', encoding='utf-8') as f:\n",
        "            saved_results = json.load(f)\n",
        "        print(f\"✓ Loaded {len(saved_results)} saved results from checkpoint.\")\n",
        "        return saved_results\n",
        "    else:\n",
        "        print(\"✓ No previous checkpoint found. Starting fresh.\")\n",
        "        return []\n",
        "\n",
        "\n",
        "# Main\n",
        "if __name__ == \"__main__\":\n",
        "    MODEL_CHOICE = \"allam\"  # Options: \"llama\", \"allam\", \"qwen\"\n",
        "\n",
        "    print(\"Loading language model...\")\n",
        "    if MODEL_CHOICE == \"llama\":\n",
        "        model_id = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
        "        llm_model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_id,\n",
        "            torch_dtype=torch.float16,\n",
        "            device_map=\"auto\"\n",
        "        )\n",
        "        llm_tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "        print(f\"Loaded Llama 3.1 model successfully\")\n",
        "\n",
        "    elif MODEL_CHOICE == \"allam\":\n",
        "        model_id = \"ALLaM-AI/ALLaM-7B-Instruct-preview\"\n",
        "        llm_tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "        llm_model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_id,\n",
        "            torch_dtype=torch.float16,\n",
        "            device_map=\"auto\"\n",
        "        )\n",
        "        print(f\"Loaded ALLaM model successfully\")\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown model choice: {MODEL_CHOICE}\")\n",
        "\n",
        "    print(\"Loading entailment model...\")\n",
        "    entailment_model = ArabicEntailmentModel()\n",
        "    print(\"Entailment model loaded successfully\")\n",
        "\n",
        "    print(\"Loading dataset...\")\n",
        "    DATASET_CHOICE = 'arabicaqa'  # Options: 'arabicaqa', 'xor_tydiqa'\n",
        "    FILE_PATH = '/content/test-open.json'\n",
        "    data = load_qa_dataset(DATASET_CHOICE, FILE_PATH)\n",
        "    print(f\"Loaded {len(data)} questions from {DATASET_CHOICE}\")\n",
        "\n",
        "    print(\"\\nStarting semantic entropy experiments...\")\n",
        "    partial_save_path = f'semantic_entropy_{MODEL_CHOICE}_partial_results.json'\n",
        "\n",
        "    # Load previous partial results if available\n",
        "    results = load_partial_results(partial_save_path)\n",
        "\n",
        "    already_processed = len(results)\n",
        "    print(f\"Starting from question {already_processed + 1}\")\n",
        "\n",
        "    MAX_QUESTIONS = None  # Set to a number or None\n",
        "    test_questions = data[:MAX_QUESTIONS] if MAX_QUESTIONS else data\n",
        "\n",
        "    for i, item in tqdm(enumerate(test_questions[already_processed:], start=already_processed), total=len(test_questions)-already_processed, desc=\"Processing questions\"):\n",
        "        question = item[\"Question\"]\n",
        "        original_answer = item[\"Answer\"]\n",
        "        print(f\"\\n[{i+1}/{len(test_questions)}] Processing question\")\n",
        "\n",
        "        result = experiment_semantic_entropy(\n",
        "            question=question,\n",
        "            original_answer=original_answer,\n",
        "            llm_model=llm_model,\n",
        "            llm_tokenizer=llm_tokenizer,\n",
        "            entailment_model=entailment_model,\n",
        "            num_samples=10\n",
        "        )\n",
        "        results.append(result)\n",
        "\n",
        "        # Save progress after every 5 questions\n",
        "        if (i+1) % 5 == 0:\n",
        "            with open(partial_save_path, 'w', encoding='utf-8') as f:\n",
        "                json.dump(results, f, ensure_ascii=False, indent=2)\n",
        "            print(f\"✓ Saved partial progress after {i+1} questions.\")\n",
        "\n",
        "    # Save final results\n",
        "    final_save_path = f'semantic_entropy_{MODEL_CHOICE}_{DATASET_CHOICE}_results.json'\n",
        "    print(\"\\nSaving final results...\")\n",
        "    with open(final_save_path, 'w', encoding='utf-8') as f:\n",
        "        json.dump(results, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    files.download(final_save_path)\n",
        "    print(f\"✓ Experiment completed successfully with {MODEL_CHOICE} model!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRo1E9seb8tM"
      },
      "source": [
        "# **extra**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ht7O3pP9bGFU"
      },
      "outputs": [],
      "source": [
        "files.download(f'semantic_entropy_{MODEL_CHOICE}_{DATASET_CHOICE}_results.json')\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyPn+DAfI8jMakt8HMw/T0pr",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}