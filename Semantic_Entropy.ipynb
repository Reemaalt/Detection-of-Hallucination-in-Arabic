{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyOz3UBcLVx6XhNbO7Z1ZvOS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3e46c81585c34081a42c76440bc2ddea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c7259ac94dfd400c82fe4c3e7ab37619",
              "IPY_MODEL_fefca50db3c64c778348bb476d168385",
              "IPY_MODEL_07dff6e60239474b9aa3f68afd653c2d"
            ],
            "layout": "IPY_MODEL_3566d66fb2cb42959127415dc4d04005"
          }
        },
        "c7259ac94dfd400c82fe4c3e7ab37619": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1c99db00a2284f33a29f373496f05337",
            "placeholder": "​",
            "style": "IPY_MODEL_bb1748922f234193a08413adf6059cd3",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "fefca50db3c64c778348bb476d168385": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1f89240f73794acfa82f2957cfe594fc",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e8aa70f0acfb47ca8e0cff16f9b169d4",
            "value": 3
          }
        },
        "07dff6e60239474b9aa3f68afd653c2d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6ac3f3df274f469fbe717c79a9b7ebbc",
            "placeholder": "​",
            "style": "IPY_MODEL_7f8ef6f147ac4cd1a49c55ff12bdf19d",
            "value": " 3/3 [00:03&lt;00:00,  1.03s/it]"
          }
        },
        "3566d66fb2cb42959127415dc4d04005": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1c99db00a2284f33a29f373496f05337": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bb1748922f234193a08413adf6059cd3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1f89240f73794acfa82f2957cfe594fc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e8aa70f0acfb47ca8e0cff16f9b169d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6ac3f3df274f469fbe717c79a9b7ebbc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7f8ef6f147ac4cd1a49c55ff12bdf19d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Reemaalt/Detection-of-Hallucination-in-Arabic/blob/main/Semantic_Entropy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S-CbnTf8Ivac",
        "outputId": "05127934-d23a-44d6-9e9e-fe77d855c421"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    To log in, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Enter your token (input will not be visible): \n",
            "Add token as git credential? (Y/n) ى\n",
            "Invalid input. Must be one of ('y', 'yes', '1', 'n', 'no', '0', '')\n",
            "Add token as git credential? (Y/n) n\n",
            "Token is valid (permission: fineGrained).\n",
            "The token `week1 test` has been saved to /root/.cache/huggingface/stored_tokens\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful.\n",
            "The current active token is: `week1 test`\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "rcBbLGoZCZUP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eea81a46-df92-4d3d-e0fd-dc6508f3700a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "# Semantic Entropy code Based on the original implementation by the new githup\n",
        "import json\n",
        "import os\n",
        "import pickle\n",
        "import random\n",
        "from functools import lru_cache\n",
        "from tqdm import tqdm\n",
        "import zipfile\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from transformers import ElectraTokenizerFast, ElectraForSequenceClassification, AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "\n",
        "# Set up device and logging\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {DEVICE}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#EntailmentModel"
      ],
      "metadata": {
        "id": "HDVS2hTwDcwg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ues our trained fine-tunied model\n",
        "#get the finetuned model from drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "zip_path = \"/content/drive/My Drive/araelectra-nli-finetuned.zip\"  # Adjust the path if needed\n",
        "extract_path = \"/content/araelectra\"\n",
        "\n",
        "# Extract the zip file\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "print(\"Extraction complete.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IOm4n6WiGCbQ",
        "outputId": "00fb7bd6-643a-45f5-9e71-3bf50c1198ed"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Extraction complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Modify the ArabicEntailmentModel class to handle dictionary arguments properly\n",
        "class ArabicEntailmentModel:\n",
        "    \"\"\"\n",
        "    Arabic entailment checker using our AraELECTRA model.\n",
        "    \"\"\"\n",
        "    def __init__(self, model_path=\"/content/araelectra/araelectra-nli-finetuned\"):\n",
        "        \"\"\"\n",
        "        Parameters:\n",
        "        - model_path: Path to the fine-tuned AraELECTRA model\n",
        "        \"\"\"\n",
        "\n",
        "        print(\"Loading AraELECTRA model for Arabic entailment checking...\")\n",
        "        self.tokenizer = ElectraTokenizerFast.from_pretrained(model_path)\n",
        "        self.model = ElectraForSequenceClassification.from_pretrained(model_path)\n",
        "        self.model = self.model.to(DEVICE)\n",
        "        self.model.eval()  # Set to evaluation mode\n",
        "        self.cache = {}  # Use a custom cache to make it faster\n",
        "        print(\"AraELECTRA model loaded successfully\")\n",
        "\n",
        "    def check_implication(self, text1, text2, *args, **kwargs):\n",
        "        \"\"\"\n",
        "        Check entailment between text1 and text2.\n",
        "\n",
        "        Parameters:\n",
        "        - text1: First text (premise)\n",
        "        - text2: Second text (hypothesis)\n",
        "\n",
        "        Returns:\n",
        "        - 0 for contradiction, 1 for neutral, 2 for entailment (matching semantic entropy code)\n",
        "        \"\"\"\n",
        "        # Create a cache key from the text inputs\n",
        "        cache_key = (text1, text2)\n",
        "        # Check if result is already in cache\n",
        "        if cache_key in self.cache:\n",
        "            return self.cache[cache_key]\n",
        "\n",
        "        # Prepare input\n",
        "        inputs = self.tokenizer(\n",
        "            text1,\n",
        "            text2,\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=128,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        # Move inputs to device\n",
        "        inputs = {k: v.to(DEVICE) for k, v in inputs.items()}\n",
        "\n",
        "        # Get prediction\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(**inputs)\n",
        "            logits = outputs.logits\n",
        "            probs = F.softmax(logits, dim=1)\n",
        "            predicted_class = torch.argmax(probs, dim=1).item()\n",
        "\n",
        "        # Map prediction to match semantic entropy code:\n",
        "        # 0: contradiction, 1: neutral, 2: entailment\n",
        "        result_map = {0: 2, 1: 1, 2: 0}  # Convert AraELECTRA output to our expected format {0: \"entailment\", 1: \"neutral\", 2: \"contradiction\"}\n",
        "        result = result_map.get(predicted_class)\n",
        "\n",
        "        # Store in cache\n",
        "        self.cache[cache_key] = result\n",
        "\n",
        "        return result"
      ],
      "metadata": {
        "id": "BtaTMvXqCvS3"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Clustering\n",
        "\n",
        "#also from their github got this\n",
        "\n",
        "```\n",
        "def get_semantic_ids(strings_list, model, strict_entailment=False, example=None):\n",
        "    \"\"\"Group list of predictions into semantic meaning.\"\"\"\n",
        "\n",
        "    def are_equivalent(text1, text2):\n",
        "\n",
        "        implication_1 = model.check_implication(text1, text2, example=example)\n",
        "        implication_2 = model.check_implication(text2, text1, example=example)  # pylint: disable=arguments-out-of-order\n",
        "        assert (implication_1 in [0, 1, 2]) and (implication_2 in [0, 1, 2])\n",
        "\n",
        "        if strict_entailment:\n",
        "            semantically_equivalent = (implication_1 == 2) and (implication_2 == 2)\n",
        "\n",
        "        else:\n",
        "            implications = [implication_1, implication_2]\n",
        "            # Check if none of the implications are 0 (contradiction) and not both of them are neutral.\n",
        "            semantically_equivalent = (0 not in implications) and ([1, 1] != implications)\n",
        "\n",
        "        return semantically_equivalent\n",
        "\n",
        "    # Initialise all ids with -1.\n",
        "    semantic_set_ids = [-1] * len(strings_list)\n",
        "    # Keep track of current id.\n",
        "    next_id = 0\n",
        "    for i, string1 in enumerate(strings_list):\n",
        "        # Check if string1 already has an id assigned.\n",
        "        if semantic_set_ids[i] == -1:\n",
        "            # If string1 has not been assigned an id, assign it next_id.\n",
        "            semantic_set_ids[i] = next_id\n",
        "            for j in range(i+1, len(strings_list)):\n",
        "                # Search through all remaining strings. If they are equivalent to string1, assign them the same id.\n",
        "                if are_equivalent(string1, strings_list[j]):\n",
        "                    semantic_set_ids[j] = next_id\n",
        "            next_id += 1\n",
        "\n",
        "    assert -1 not in semantic_set_ids\n",
        "\n",
        "    return semantic_set_ids\n",
        "\n",
        "\n",
        "def logsumexp_by_id(semantic_ids, log_likelihoods, agg='sum_normalized'):\n",
        "    \"\"\"Sum probabilities with the same semantic id.\n",
        "\n",
        "    Log-Sum-Exp because input and output probabilities in log space.\n",
        "    \"\"\"\n",
        "    unique_ids = sorted(list(set(semantic_ids)))\n",
        "    assert unique_ids == list(range(len(unique_ids)))\n",
        "    log_likelihood_per_semantic_id = []\n",
        "\n",
        "    for uid in unique_ids:\n",
        "        # Find positions in `semantic_ids` which belong to the active `uid`.\n",
        "        id_indices = [pos for pos, x in enumerate(semantic_ids) if x == uid]\n",
        "        # Gather log likelihoods at these indices.\n",
        "        id_log_likelihoods = [log_likelihoods[i] for i in id_indices]\n",
        "        if agg == 'sum_normalized':\n",
        "            # log_lik_norm = id_log_likelihoods - np.prod(log_likelihoods)\n",
        "            log_lik_norm = id_log_likelihoods - np.log(np.sum(np.exp(log_likelihoods)))\n",
        "            logsumexp_value = np.log(np.sum(np.exp(log_lik_norm)))\n",
        "        else:\n",
        "            raise ValueError\n",
        "        log_likelihood_per_semantic_id.append(logsumexp_value)\n",
        "\n",
        "    return log_likelihood_per_semantic_id\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "ulsao1Uw5jKm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_semantic_ids(strings_list, model, strict_entailment=False, example=None):\n",
        "\n",
        "    # Group list of predictions into semantic meaning\n",
        "\n",
        "    def are_equivalent(text1, text2):\n",
        "        # Check if text1 entails text2\n",
        "        implication_1 = model.check_implication(text1, text2, example=example)\n",
        "        # Check if text2 entails text1\n",
        "        implication_2 = model.check_implication(text2, text1, example=example)\n",
        "        assert (implication_1 in [0, 1, 2]) and (implication_2 in [0, 1, 2])\n",
        "\n",
        "        if strict_entailment:\n",
        "            # Both must indicate entailment (2) for semantic equivalence\n",
        "            semantically_equivalent = (implication_1 == 2) and (implication_2 == 2)\n",
        "        else:\n",
        "            implications = [implication_1, implication_2]\n",
        "            # Check if none of the implications are 0 (contradiction) and not both of them are neutral.)\n",
        "            semantically_equivalent = (0 not in implications) and ([1, 1] != implications)\n",
        "\n",
        "        return semantically_equivalent\n",
        "\n",
        "    # Initialize all ids with -1\n",
        "    semantic_set_ids = [-1] * len(strings_list)\n",
        "    # Keep track of current id\n",
        "    next_id = 0\n",
        "\n",
        "    for i, string1 in enumerate(strings_list):\n",
        "        # Check if string1 already has an id assigned\n",
        "        if semantic_set_ids[i] == -1:\n",
        "            # If string1 has not been assigned an id, assign it next_id\n",
        "            semantic_set_ids[i] = next_id\n",
        "            for j in range(i+1, len(strings_list)):\n",
        "                # Search through all remaining strings. If they are equivalent to string1, assign them the same id.\n",
        "                if semantic_set_ids[j] == -1 and are_equivalent(string1, strings_list[j]):\n",
        "                    semantic_set_ids[j] = next_id\n",
        "            next_id += 1\n",
        "\n",
        "    assert -1 not in semantic_set_ids\n",
        "\n",
        "    return semantic_set_ids\n",
        "\n",
        "def logsumexp_by_id(semantic_ids, log_likelihoods, agg='sum_normalized'):\n",
        "    \"\"\"\n",
        "    Sum probabilities with the same semantic ID.\n",
        "    Log-Sum-Exp because input and output probabilities in log space.\n",
        "    \"\"\"\n",
        "    unique_ids = sorted(list(set(semantic_ids)))\n",
        "    assert unique_ids == list(range(len(unique_ids)))\n",
        "    log_likelihood_per_semantic_id = []\n",
        "\n",
        "    for uid in unique_ids:\n",
        "        # Find positions in `semantic_ids` which belong to the active `uid`\n",
        "        id_indices = [pos for pos, x in enumerate(semantic_ids) if x == uid]\n",
        "        # Gather log likelihoods at these indices\n",
        "        id_log_likelihoods = [log_likelihoods[i] for i in id_indices]\n",
        "\n",
        "        if agg == 'sum_normalized':\n",
        "            # Normalize by subtracting the log sum exp of all log likelihoods\n",
        "           # log_lik_norm = id_log_likelihoods - np.prod(log_likelihoods)\n",
        "            log_lik_norm = id_log_likelihoods - np.log(np.sum(np.exp(log_likelihoods)))\n",
        "            logsumexp_value = np.log(np.sum(np.exp(log_lik_norm)))\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown aggregation method: {agg}\")\n",
        "\n",
        "        log_likelihood_per_semantic_id.append(logsumexp_value)\n",
        "\n",
        "    return log_likelihood_per_semantic_id"
      ],
      "metadata": {
        "id": "NW3ag2a4LHhN"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#entropy\n",
        "\n",
        "#from their github i got this code\n",
        "\n",
        "```\n",
        "\n",
        "def predictive_entropy(log_probs):\n",
        "    \"\"\"Compute MC estimate of entropy.\n",
        "\n",
        "    `E[-log p(x)] ~= -1/N sum_i log p(x_i)`, i.e. the average token likelihood.\n",
        "    \"\"\"\n",
        "\n",
        "    entropy = -np.sum(log_probs) / len(log_probs)\n",
        "\n",
        "    return entropy\n",
        "\n",
        "\n",
        "def predictive_entropy_rao(log_probs):\n",
        "    entropy = -np.sum(np.exp(log_probs) * log_probs)\n",
        "    return entropy\n",
        "\n",
        "\n",
        "def cluster_assignment_entropy(semantic_ids):\n",
        "    \"\"\"Estimate semantic uncertainty from how often different clusters get assigned.\n",
        "\n",
        "    We estimate the categorical distribution over cluster assignments from the\n",
        "    semantic ids. The uncertainty is then given by the entropy of that\n",
        "    distribution. This estimate does not use token likelihoods, it relies soley\n",
        "    on the cluster assignments. If probability mass is spread of between many\n",
        "    clusters, entropy is larger. If probability mass is concentrated on a few\n",
        "    clusters, entropy is small.\n",
        "\n",
        "    Input:\n",
        "        semantic_ids: List of semantic ids, e.g. [0, 1, 2, 1].\n",
        "    Output:\n",
        "        cluster_entropy: Entropy, e.g. (-p log p).sum() for p = [1/4, 2/4, 1/4].\n",
        "    \"\"\"\n",
        "\n",
        "    n_generations = len(semantic_ids)\n",
        "    counts = np.bincount(semantic_ids)\n",
        "    probabilities = counts/n_generations\n",
        "    assert np.isclose(probabilities.sum(), 1)\n",
        "    entropy = - (probabilities * np.log(probabilities)).sum()\n",
        "    return entropy\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "H2wHskqM9lkQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predictive_entropy_rao(log_probs):\n",
        "    \"\"\"\n",
        "    Compute entropy from log probabilities.\n",
        "\n",
        "    Parameters:\n",
        "    - log_probs: Log probabilities\n",
        "\n",
        "    Returns:\n",
        "    - Entropy value\n",
        "    \"\"\"\n",
        "    entropy = -np.sum(np.exp(log_probs) * log_probs)\n",
        "    return entropy\n",
        "\n",
        "def predictive_entropy(log_probs):\n",
        "    \"\"\"Compute MC estimate of entropy.\n",
        "\n",
        "    `E[-log p(x)] ~= -1/N sum_i log p(x_i)`, i.e. the average token likelihood.\n",
        "    \"\"\"\n",
        "\n",
        "    entropy = -np.sum(log_probs) / len(log_probs)\n",
        "\n",
        "    return entropy"
      ],
      "metadata": {
        "id": "Euc93oDFGKCa"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# call function\n",
        "The actual full computation in the original repo happens in compute_uncertainty_measures.py:\n",
        " This  code is where they call the functions to compute the entropy measures\n",
        "\n",
        "```\n",
        "\n",
        "if args.compute_predictive_entropy:\n",
        "    # Token log likelihoods. Shape = (n_sample, n_tokens)\n",
        "    if not args.use_all_generations:\n",
        "        log_liks = [r[1] for r in full_responses[:args.use_num_generations]]\n",
        "    else:\n",
        "        log_liks = [r[1] for r in full_responses]\n",
        "\n",
        "    for i in log_liks:\n",
        "        assert i\n",
        "\n",
        "    if args.compute_context_entails_response:\n",
        "        # Compute context entails answer baseline.\n",
        "        entropies['context_entails_response'].append(context_entails_response(\n",
        "            context, responses, entailment_model))\n",
        "\n",
        "    if args.condition_on_question and args.entailment_model == 'deberta':\n",
        "        responses = [f'{question} {r}' for r in responses]\n",
        "\n",
        "    # Compute semantic ids.\n",
        "    semantic_ids = get_semantic_ids(\n",
        "        responses, model=entailment_model,\n",
        "        strict_entailment=args.strict_entailment, example=example)\n",
        "\n",
        "    result_dict['semantic_ids'].append(semantic_ids)\n",
        "\n",
        "    # Compute entropy from frequencies of cluster assignments.\n",
        "    entropies['cluster_assignment_entropy'].append(cluster_assignment_entropy(semantic_ids))\n",
        "\n",
        "    # Length normalization of generation probabilities.\n",
        "    log_liks_agg = [np.mean(log_lik) for log_lik in log_liks]\n",
        "\n",
        "    # Compute naive entropy.\n",
        "    entropies['regular_entropy'].append(predictive_entropy(log_liks_agg))\n",
        "\n",
        "    # Compute semantic entropy.\n",
        "    log_likelihood_per_semantic_id = logsumexp_by_id(semantic_ids, log_liks_agg, agg='sum_normalized')\n",
        "    pe = predictive_entropy_rao(log_likelihood_per_semantic_id)\n",
        "    entropies['semantic_entropy'].append(pe)\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "OnKq6Ixs9iSr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def experiment_semantic_entropy(question, llm_model, llm_tokenizer, entailment_model, num_samples=10):\n",
        "    \"\"\"\n",
        "    Compute semantic entropy for a given question.\n",
        "    Returns:\n",
        "        Dictionary containing results of the experiment\n",
        "    \"\"\"\n",
        "    # Step 1: Generate multiple answers with their token log likelihoods\n",
        "    print(f\"Generating {num_samples} answers for question: {question}\")\n",
        "    results = generate_answer(question, num_samples, llm_model, llm_tokenizer)\n",
        "\n",
        "    # Step 2: Extract answers and their log likelihoods\n",
        "    answers = [result['text'] for result in results]\n",
        "    # length normalization of the log probabilities\n",
        "    log_likelihoods = [np.mean(result['token_log_probs']) for result in results]\n",
        "\n",
        "    print(f\"Generated {len(answers)} answers\")\n",
        "\n",
        "    # Step 3: Create an example dictionary for entailment checking\n",
        "    example = {'question': question}\n",
        "\n",
        "    # Step 4: Compute semantic clusters\n",
        "    print(\"Computing semantic clusters...\")\n",
        "    semantic_ids = get_semantic_ids(answers, entailment_model, strict_entailment=False, example=example)\n",
        "    unique_clusters = len(set(semantic_ids))\n",
        "    print(f\"Found {unique_clusters} semantic clusters\")\n",
        "\n",
        "    # Step 5: Calculate entropy measures\n",
        "    # Regular entropy calculation (based on log likelihoods only)\n",
        "    regular_entropy = predictive_entropy(log_likelihoods)\n",
        "\n",
        "    # Semantic entropy calculation (based on semantic clusters and log likelihoods)\n",
        "    log_likelihood_per_semantic_id = logsumexp_by_id(semantic_ids, log_likelihoods, agg='sum_normalized')\n",
        "    semantic_entropy = predictive_entropy_rao(log_likelihood_per_semantic_id)\n",
        "\n",
        "    # Step 6: Print results\n",
        "    print(f\"\\nEntropy Analysis for: '{question}'\")\n",
        "    print(f\"Generated {len(answers)} answers in {unique_clusters} semantic clusters\")\n",
        "    #print the entropy values with 4 decimal places\n",
        "    print(f\" Regular Entropy: {regular_entropy:.4f}\")\n",
        "    print(f\"Semantic Entropy: {semantic_entropy:.4f}\")\n",
        "\n",
        "\n",
        "    # Step 6: Display cluster information\n",
        "    print(\"\\nSemantic Clusters:\")\n",
        "    unique_clusters = sorted(list(set(semantic_ids)))\n",
        "    for cluster_id in unique_clusters:\n",
        "        cluster_items = [answers[i] for i, sid in enumerate(semantic_ids) if sid == cluster_id]\n",
        "        count = len(cluster_items)\n",
        "        print(f\"\\nCluster {cluster_id} ({count} items):\")\n",
        "        for item in cluster_items:\n",
        "            print(f\"  - {item}\")\n",
        "\n",
        "    # Return  results\n",
        "    # number like 2.220446049250313e-16 is extremely close to zero (basically floating-point noise).\n",
        "    # so rounds the number to 4 decimal places\n",
        "    return {\n",
        "    'question': question,\n",
        "    'answers': answers,\n",
        "    'log_likelihoods': log_likelihoods,\n",
        "    'semantic_ids': semantic_ids,\n",
        "    'regular_entropy': round(regular_entropy, 4),\n",
        "    'semantic_entropy': round(semantic_entropy, 4),\n",
        "    'num_clusters': unique_clusters\n",
        "\n",
        "}\n"
      ],
      "metadata": {
        "id": "_uy0aCxkIL_R"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#get likelihood\n",
        "\n",
        "#from their code\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "#at this function\n",
        "def predict(self, input_data, temperature, return_full=False):\n",
        "\n",
        "\n",
        "        # Get log_likelihoods.\n",
        "        # outputs.scores are the logits for the generated token.\n",
        "        # outputs.scores is a tuple of len = n_generated_tokens.\n",
        "        # Each entry is shape (bs, vocabulary size).\n",
        "        # outputs.sequences is the sequence of all tokens: input and generated.\n",
        "        transition_scores = self.model.compute_transition_scores(\n",
        "            outputs.sequences, outputs.scores, normalize_logits=True)\n",
        "        # Transition_scores[0] only contains the scores for the first generated tokens.\n",
        "\n",
        "        log_likelihoods = [score.item() for score in transition_scores[0]]\n",
        "        if len(log_likelihoods) == 1:\n",
        "            logging.warning('Taking first and only generation for log likelihood!')\n",
        "            log_likelihoods = log_likelihoods\n",
        "        else:\n",
        "            log_likelihoods = log_likelihoods[:n_generated]\n",
        "\n",
        "        if len(log_likelihoods) == self.max_new_tokens:\n",
        "            logging.warning('Generation interrupted by max_token limit.')\n",
        "\n",
        "        if len(log_likelihoods) == 0:\n",
        "            raise ValueError\n",
        "\n",
        "        return sliced_answer, log_likelihoods, last_token_embedding\n",
        "\n",
        "\n",
        "all function if needed\n",
        "f predict(self, input_data, temperature, return_full=False):\n",
        "\n",
        "        # Implement prediction.\n",
        "        inputs = self.tokenizer(input_data, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "        if 'llama' in self.model_name.lower() or 'falcon' in self.model_name or 'mistral' in self.model_name.lower():\n",
        "            if 'token_type_ids' in inputs:  # Some HF models have changed.\n",
        "                del inputs['token_type_ids']\n",
        "            pad_token_id = self.tokenizer.eos_token_id\n",
        "        else:\n",
        "            pad_token_id = None\n",
        "\n",
        "        if self.stop_sequences is not None:\n",
        "            stopping_criteria = StoppingCriteriaList([StoppingCriteriaSub(\n",
        "                stops=self.stop_sequences,\n",
        "                initial_length=len(inputs['input_ids'][0]),\n",
        "                tokenizer=self.tokenizer)])\n",
        "        else:\n",
        "            stopping_criteria = None\n",
        "\n",
        "        logging.debug('temperature: %f', temperature)\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=self.max_new_tokens,\n",
        "                return_dict_in_generate=True,\n",
        "                output_scores=True,\n",
        "                output_hidden_states=True,\n",
        "                temperature=temperature,\n",
        "                do_sample=True,\n",
        "                stopping_criteria=stopping_criteria,\n",
        "                pad_token_id=pad_token_id,\n",
        "            )\n",
        "\n",
        "        if len(outputs.sequences[0]) > self.token_limit:\n",
        "            raise ValueError(\n",
        "                'Generation exceeding token limit %d > %d',\n",
        "                len(outputs.sequences[0]), self.token_limit)\n",
        "\n",
        "        full_answer = self.tokenizer.decode(\n",
        "            outputs.sequences[0], skip_special_tokens=True)\n",
        "\n",
        "        if return_full:\n",
        "            return full_answer\n",
        "\n",
        "        # For some models, we need to remove the input_data from the answer.\n",
        "        if full_answer.startswith(input_data):\n",
        "            input_data_offset = len(input_data)\n",
        "        else:\n",
        "            raise ValueError('Have not tested this in a while.')\n",
        "\n",
        "        # Remove input from answer.\n",
        "        answer = full_answer[input_data_offset:]\n",
        "\n",
        "        # Remove stop_words from answer.\n",
        "        stop_at = len(answer)\n",
        "        sliced_answer = answer\n",
        "        if self.stop_sequences is not None:\n",
        "            for stop in self.stop_sequences:\n",
        "                if answer.endswith(stop):\n",
        "                    stop_at = len(answer) - len(stop)\n",
        "                    sliced_answer = answer[:stop_at]\n",
        "                    break\n",
        "            if not all([stop not in sliced_answer for stop in self.stop_sequences]):\n",
        "                error_msg = 'Error: Stop words not removed successfully!'\n",
        "                error_msg += f'Answer: >{answer}< '\n",
        "                error_msg += f'Sliced Answer: >{sliced_answer}<'\n",
        "                if 'falcon' not in self.model_name.lower():\n",
        "                    raise ValueError(error_msg)\n",
        "                else:\n",
        "                    logging.error(error_msg)\n",
        "\n",
        "        # Remove whitespaces from answer (in particular from beginning.)\n",
        "        sliced_answer = sliced_answer.strip()\n",
        "\n",
        "        # Get the number of tokens until the stop word comes up.\n",
        "        # Note: Indexing with `stop_at` already excludes the stop_token.\n",
        "        # Note: It's important we do this with full answer, since there might be\n",
        "        # non-trivial interactions between the input_data and generated part\n",
        "        # in tokenization (particularly around whitespaces.)\n",
        "        token_stop_index = self.tokenizer(full_answer[:input_data_offset + stop_at], return_tensors=\"pt\")['input_ids'].shape[1]\n",
        "        n_input_token = len(inputs['input_ids'][0])\n",
        "        n_generated = token_stop_index - n_input_token\n",
        "\n",
        "        if n_generated == 0:\n",
        "            logging.warning('Only stop_words were generated. For likelihoods and embeddings, taking stop word instead.')\n",
        "            n_generated = 1\n",
        "\n",
        "        # Get the last hidden state (last layer) and the last token's embedding of the answer.\n",
        "        # Note: We do not want this to be the stop token.\n",
        "\n",
        "        # outputs.hidden_state is a tuple of len = n_generated_tokens.\n",
        "        # The first hidden state is for the input tokens and is of shape\n",
        "        #     (n_layers) x (batch_size, input_size, hidden_size).\n",
        "        # (Note this includes the first generated token!)\n",
        "        # The remaining hidden states are for the remaining generated tokens and is of shape\n",
        "        #    (n_layers) x (batch_size, 1, hidden_size).\n",
        "\n",
        "        # Note: The output embeddings have the shape (batch_size, generated_length, hidden_size).\n",
        "        # We do not get embeddings for input_data! We thus subtract the n_tokens_in_input from\n",
        "        # token_stop_index to arrive at the right output.\n",
        "\n",
        "        if 'decoder_hidden_states' in outputs.keys():\n",
        "            hidden = outputs.decoder_hidden_states\n",
        "        else:\n",
        "            hidden = outputs.hidden_states\n",
        "\n",
        "        if len(hidden) == 1:\n",
        "            logging.warning(\n",
        "                'Taking first and only generation for hidden! '\n",
        "                'n_generated: %d, n_input_token: %d, token_stop_index %d, '\n",
        "                'last_token: %s, generation was: %s',\n",
        "                n_generated, n_input_token, token_stop_index,\n",
        "                self.tokenizer.decode(outputs['sequences'][0][-1]),\n",
        "                full_answer,\n",
        "                )\n",
        "            last_input = hidden[0]\n",
        "        elif ((n_generated - 1) >= len(hidden)):\n",
        "            # If access idx is larger/equal.\n",
        "            logging.error(\n",
        "                'Taking last state because n_generated is too large'\n",
        "                'n_generated: %d, n_input_token: %d, token_stop_index %d, '\n",
        "                'last_token: %s, generation was: %s, slice_answer: %s',\n",
        "                n_generated, n_input_token, token_stop_index,\n",
        "                self.tokenizer.decode(outputs['sequences'][0][-1]),\n",
        "                full_answer, sliced_answer\n",
        "                )\n",
        "            last_input = hidden[-1]\n",
        "        else:\n",
        "            last_input = hidden[n_generated - 1]\n",
        "\n",
        "        # Then access last layer for input\n",
        "        last_layer = last_input[-1]\n",
        "        # Then access last token in input.\n",
        "        last_token_embedding = last_layer[:, -1, :].cpu()\n",
        "\n",
        "        # Get log_likelihoods.\n",
        "        # outputs.scores are the logits for the generated token.\n",
        "        # outputs.scores is a tuple of len = n_generated_tokens.\n",
        "        # Each entry is shape (bs, vocabulary size).\n",
        "        # outputs.sequences is the sequence of all tokens: input and generated.\n",
        "        transition_scores = self.model.compute_transition_scores(\n",
        "            outputs.sequences, outputs.scores, normalize_logits=True)\n",
        "        # Transition_scores[0] only contains the scores for the first generated tokens.\n",
        "\n",
        "        log_likelihoods = [score.item() for score in transition_scores[0]]\n",
        "        if len(log_likelihoods) == 1:\n",
        "            logging.warning('Taking first and only generation for log likelihood!')\n",
        "            log_likelihoods = log_likelihoods\n",
        "        else:\n",
        "            log_likelihoods = log_likelihoods[:n_generated]\n",
        "\n",
        "        if len(log_likelihoods) == self.max_new_tokens:\n",
        "            logging.warning('Generation interrupted by max_token limit.')\n",
        "\n",
        "        if len(log_likelihoods) == 0:\n",
        "            raise ValueError\n",
        "\n",
        "        return sliced_answer, log_likelihoods, last_token_embedding\n",
        "\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "a0EoVG07fDNN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##log probabilities vs negative log likelihoods:\n",
        "\n",
        "- Log probabilities are the logarithm of the probability: log(p)\n",
        "- Negative log likelihoods are the negative logarithm of the probability: -log(p)\n",
        "\n",
        "The original code is working with log probabilities, not negative log likelihoods. We see normalize_logits=True in compute_transition_scores, it means the model is returning log probabilities.\n",
        "\n",
        "\n",
        "**The original code is using predictive_entropy_rao, which expects log probabilities as input."
      ],
      "metadata": {
        "id": "LFYlG7k_gCIS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "نوف تبي\n",
        "\n",
        "        # Then access last layer for input\n",
        "        last_layer = last_input[-1]\n",
        "        # Then access last token in input.\n",
        "        last_token_embedding = last_layer[:, -1, :].cpu()\n",
        "      return sliced_answer, log_likelihoods, last_token_embedding"
      ],
      "metadata": {
        "id": "sziR1jcAFFeI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_answer(question, num_samples, model, tokenizer):\n",
        "    \"\"\"\n",
        "    Generate multiple answers to a question using LLM with direct token log likelihoods.\n",
        "    \"\"\"\n",
        "    # Create the prompt with the question\n",
        "    prompt = f\"أجب على السؤال التالي بجملة واحدة فقط موجزة ولكن كاملة باللغة العربية\\nQuestion: {question}\\nAnswer:\"\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for _ in range(num_samples):\n",
        "        # Tokenize the prompt\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "        prompt_length = inputs.input_ids.shape[1]  # Number of tokens in the prompt\n",
        "\n",
        "        # Generate with return_dict_in_generate=True and output_scores=True to get scores\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                inputs.input_ids,\n",
        "                max_new_tokens=100,\n",
        "                do_sample=True,\n",
        "                temperature=0.5,\n",
        "                return_dict_in_generate=True,\n",
        "                output_scores=True,\n",
        "            )\n",
        "\n",
        "        # Calculate token log probabilities using compute_transition_scores\n",
        "        # normalize_logits=True ensures we get log probabilities\n",
        "        transition_scores = model.compute_transition_scores(\n",
        "            outputs.sequences,\n",
        "            outputs.scores,\n",
        "            normalize_logits=True\n",
        "        )\n",
        "\n",
        "        # Extract log likelihoods like they did exactly, but no handeling of cases\n",
        "        log_likelihoods = [score.item() for score in transition_scores[0]]\n",
        "\n",
        "\n",
        "        # Decode the generated text\n",
        "        generated_text = tokenizer.decode(outputs.sequences[0], skip_special_tokens=True)\n",
        "        answer = generated_text.split(\"Answer:\")[-1].strip()\n",
        "\n",
        "        # Clean the output\n",
        "        strings_to_filter_on = ['.', '\\n', 'Q:', 'A:', 'question:', 'answer:', 'Question:', 'Answer:',\n",
        "                               'Questions:', 'questions:', 'QUESTION:', 'ANSWER:']\n",
        "        for string in strings_to_filter_on:\n",
        "            if string in answer:\n",
        "                answer = answer.split(string)[0].strip()\n",
        "\n",
        "        results.append({\n",
        "            'text': answer,\n",
        "            'token_log_probs': log_likelihoods  # Store raw log probabilities\n",
        "        })\n",
        "\n",
        "    return results  # Return results AFTER the loop completes"
      ],
      "metadata": {
        "id": "JXUkp0AdvHBb"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main"
      ],
      "metadata": {
        "id": "KKCz4R7jnK7D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_qa_dataset(dataset_name, file_path=None):\n",
        "    data = []\n",
        "\n",
        "    try:\n",
        "        if dataset_name == 'arabicaqa' and file_path:\n",
        "            if os.path.exists(file_path):  # Use the loaded file\n",
        "                print(\"Using ArabicaQA\")\n",
        "                with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                    custom_data = json.load(f)\n",
        "                    for idx, item in enumerate(custom_data):\n",
        "                        data.append({\n",
        "                            \"question_id\": idx,\n",
        "                            \"Question\": item[\"question\"],\n",
        "                            \"Answer\": item[\"answer\"]\n",
        "                        })\n",
        "            else:\n",
        "                raise FileNotFoundError(f\"ArabicaQA file not found at {file_path}\")\n",
        "\n",
        "        elif dataset_name == 'xor_tydiqa' and file_path:\n",
        "            if os.path.exists(file_path):  # Use the loaded file\n",
        "                print(\"Using XOR-TyDiQA\")\n",
        "                print(\"Filtering Arabic QA pairs from XOR-TyDi...\")\n",
        "\n",
        "                # Load the jsonl dataset\n",
        "                with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                    custom_data = [json.loads(line) for line in f]  # Handling jsonl\n",
        "\n",
        "                # Filter Arabic samples (\"lang\" = \"ar\")\n",
        "                arabic_data = []\n",
        "                for idx, item in enumerate(custom_data):\n",
        "                    if item[\"lang\"] == \"ar\":  # Arabic language code\n",
        "                        arabic_data.append({\n",
        "                            \"question_id\": item[\"id\"],\n",
        "                            \"Question\": item[\"question\"],\n",
        "                            \"Answer\": item[\"answers\"][0]  # First answer in list\n",
        "                        })\n",
        "\n",
        "                data.extend(arabic_data)\n",
        "            else:\n",
        "                raise FileNotFoundError(f\"XOR-TyDiQA file not found at {file_path}\")\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported dataset: {dataset_name}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading {dataset_name}: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "    return data"
      ],
      "metadata": {
        "id": "a8ag496J_opE"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main\n",
        "if __name__ == \"__main__\":\n",
        "    # Choose which model to use\n",
        "    MODEL_CHOICE = \"allam\"  # Options: \"llama\", \"allam\"\n",
        "\n",
        "    # Step 1: Load the LLM model\n",
        "    print(\"Loading language model...\")\n",
        "    if MODEL_CHOICE == \"llama\":\n",
        "        model_id = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
        "        llm_model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_id,\n",
        "            torch_dtype=torch.float16,\n",
        "            device_map=\"auto\"\n",
        "        )\n",
        "        llm_tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "        print(f\"Loaded Llama 3.1 model successfully\")\n",
        "\n",
        "    elif MODEL_CHOICE == \"allam\":\n",
        "        model_id = \"ALLaM-AI/ALLaM-7B-Instruct-preview\"\n",
        "        llm_tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "        llm_model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_id,\n",
        "            torch_dtype=torch.float16,\n",
        "            device_map=\"auto\"\n",
        "        )\n",
        "        print(f\"Loaded ALLaM model successfully\")\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown model choice: {MODEL_CHOICE}\")\n",
        "\n",
        "    # Step 2: Load the entailment model\n",
        "    print(\"Loading entailment model...\")\n",
        "    entailment_model = ArabicEntailmentModel()\n",
        "    print(\"Entailment model loaded successfully\")\n",
        "\n",
        "    # Step 3: Load questions\n",
        "    print(\"Loading dataset...\")\n",
        "    DATASET_CHOICE = 'xor_tydiqa'  # Options: 'arabicaqa', 'xor_tydiqa'\n",
        "\n",
        "    FILE_PATH = '/content/xor_dev_full_v1_1.jsonl'\n",
        "\n",
        "    data = load_qa_dataset(DATASET_CHOICE, FILE_PATH)\n",
        "    questions = [item[\"Question\"] for item in data]\n",
        "    print(f\"Loaded {len(questions)} questions from {DATASET_CHOICE}\")\n",
        "\n",
        "    # Step 4: Compute semantic entropy for each question\n",
        "    print(\"\\nStarting semantic entropy experiments...\")\n",
        "    results = []\n",
        "\n",
        "    #  Limit number of questions for testing\n",
        "    MAX_QUESTIONS = 10  # Set to a number  or None for all questions\n",
        "    test_questions = questions[:MAX_QUESTIONS] if MAX_QUESTIONS else questions\n",
        "\n",
        "    for i, question in enumerate(test_questions):\n",
        "        print(f\"\\n[{i+1}/{len(test_questions)}] Processing question\")\n",
        "        result = experiment_semantic_entropy(question, llm_model, llm_tokenizer, entailment_model, num_samples=10)\n",
        "        results.append(result)\n",
        "\n",
        "        # Save intermediate results every 5 questions\n",
        "        if (i+1) % 5 == 0:\n",
        "            with open(f'semantic_entropy_{MODEL_CHOICE}_results_partial_{i+1}.json', 'w', encoding='utf-8') as f:\n",
        "                json.dump(results, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    # Step 5: Save final results\n",
        "    print(\"\\nSaving final results...\")\n",
        "    with open(f'semantic_entropy_{MODEL_CHOICE}_results.json', 'w', encoding='utf-8') as f:\n",
        "        json.dump(results, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    print(f\"✓ Experiment completed successfully with {MODEL_CHOICE} model!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "3e46c81585c34081a42c76440bc2ddea",
            "c7259ac94dfd400c82fe4c3e7ab37619",
            "fefca50db3c64c778348bb476d168385",
            "07dff6e60239474b9aa3f68afd653c2d",
            "3566d66fb2cb42959127415dc4d04005",
            "1c99db00a2284f33a29f373496f05337",
            "bb1748922f234193a08413adf6059cd3",
            "1f89240f73794acfa82f2957cfe594fc",
            "e8aa70f0acfb47ca8e0cff16f9b169d4",
            "6ac3f3df274f469fbe717c79a9b7ebbc",
            "7f8ef6f147ac4cd1a49c55ff12bdf19d"
          ]
        },
        "id": "5B6fdiUCITVs",
        "outputId": "59bb703c-afaf-447e-9a7e-ee9c4005af1e",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading language model...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3e46c81585c34081a42c76440bc2ddea"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the cpu.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded ALLaM model successfully\n",
            "Loading entailment model...\n",
            "Loading AraELECTRA model for Arabic entailment checking...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AraELECTRA model loaded successfully\n",
            "Entailment model loaded successfully\n",
            "Loading dataset...\n",
            "Using XOR-TyDiQA\n",
            "Filtering Arabic QA pairs from XOR-TyDi...\n",
            "Loaded 708 questions from xor_tydiqa\n",
            "\n",
            "Starting semantic entropy experiments...\n",
            "\n",
            "[1/10] Processing question\n",
            "Generating 10 answers for question: ما هي أولى جامعات فنلندا؟\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated 10 answers\n",
            "Computing semantic clusters...\n",
            "Found 1 semantic clusters\n",
            "\n",
            "Entropy Analysis for: 'ما هي أولى جامعات فنلندا؟'\n",
            "Generated 10 answers in 1 semantic clusters\n",
            " Regular Entropy: 0.0727\n",
            "Semantic Entropy: -0.0000\n",
            "\n",
            "Semantic Clusters:\n",
            "\n",
            "Cluster 0 (10 items):\n",
            "  - جامعة هلسنكي وجامعة توركو\n",
            "  - جامعة هلسنكي\n",
            "  - جامعة هلسنكي وجامعة توركو\n",
            "  - جامعة هلسنكي وجامعة توركو\n",
            "  - جامعة هلسنكي وجامعة توركو\n",
            "  - جامعة هلسنكي وجامعة توركو\n",
            "  - جامعة هلسنكي وجامعة توركو\n",
            "  - جامعة هلسنكي\n",
            "  - جامعة هلسنكي وجامعة توركو\n",
            "  - جامعة هلسنكي\n",
            "\n",
            "[2/10] Processing question\n",
            "Generating 10 answers for question: ما عدد الدول المطلة على بحر البلطيق؟\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated 10 answers\n",
            "Computing semantic clusters...\n",
            "Found 1 semantic clusters\n",
            "\n",
            "Entropy Analysis for: 'ما عدد الدول المطلة على بحر البلطيق؟'\n",
            "Generated 10 answers in 1 semantic clusters\n",
            " Regular Entropy: 0.1557\n",
            "Semantic Entropy: 0.0000\n",
            "\n",
            "Semantic Clusters:\n",
            "\n",
            "Cluster 0 (10 items):\n",
            "  - هناك 12 دولة مطلة على بحر البلطيق\n",
            "  - هناك 12 دولة تطل على بحر البلطيق\n",
            "  - هناك 12 دولة تطل على بحر البلطيق\n",
            "  - هناك 12 دولة تطل على بحر البلطيق\n",
            "  - هناك 12 دولة\n",
            "  - هناك 12 دولة مطلة على بحر البلطيق\n",
            "  - هناك 12 دولة مطلة على بحر البلطيق\n",
            "  - هناك 12 دولة\n",
            "  - 12 دولة\n",
            "  - هناك 12 دولة\n",
            "\n",
            "[3/10] Processing question\n",
            "Generating 10 answers for question: اين عاش نيوتن؟\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated 10 answers\n",
            "Computing semantic clusters...\n",
            "Found 1 semantic clusters\n",
            "\n",
            "Entropy Analysis for: 'اين عاش نيوتن؟'\n",
            "Generated 10 answers in 1 semantic clusters\n",
            " Regular Entropy: 0.0843\n",
            "Semantic Entropy: 0.0000\n",
            "\n",
            "Semantic Clusters:\n",
            "\n",
            "Cluster 0 (10 items):\n",
            "  - عاش إسحاق نيوتن في إنجلترا\n",
            "  - عاش إسحاق نيوتن في إنجلترا\n",
            "  - عاش نيوتن في إنجلترا\n",
            "  - عاش إسحاق نيوتن في إنجلترا\n",
            "  - عاش إسحاق نيوتن في إنجلترا\n",
            "  - عاش إسحاق نيوتن في إنجلترا\n",
            "  - عاش إسحاق نيوتن في إنجلترا\n",
            "  - عاش إسحاق نيوتن في إنجلترا\n",
            "  - عاش إسحاق نيوتن في إنجلترا\n",
            "  - عاش السير إسحاق نيوتن في إنجلترا\n",
            "\n",
            "[4/10] Processing question\n",
            "Generating 10 answers for question: هل زار ابن بطوطة اليمن؟\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **extra**"
      ],
      "metadata": {
        "id": "IRo1E9seb8tM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(f'semantic_entropy_{MODEL_CHOICE}_results.json')\n"
      ],
      "metadata": {
        "id": "Ht7O3pP9bGFU"
      },
      "execution_count": 17,
      "outputs": []
    }
  ]
}