{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Reemaalt/Detection-of-Hallucination-in-Arabic/blob/main/Semantic_Entropy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S-CbnTf8Ivac",
        "outputId": "a9976073-22e4-4589-a27c-4d7a66cbc940"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    To log in, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Enter your token (input will not be visible): \n",
            "Add token as git credential? (Y/n) ى\n",
            "Invalid input. Must be one of ('y', 'yes', '1', 'n', 'no', '0', '')\n",
            "Add token as git credential? (Y/n) n\n",
            "Token is valid (permission: fineGrained).\n",
            "The token `week1 test` has been saved to /root/.cache/huggingface/stored_tokens\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful.\n",
            "The current active token is: `week1 test`\n"
          ]
        }
      ],
      "source": [
        "!huggingface-cli login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rcBbLGoZCZUP",
        "outputId": "d5033080-04cf-4a36-940a-2c1284989610"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "# Semantic Entropy code Based on the original implementation by the new githup\n",
        "import json\n",
        "import os\n",
        "import pickle\n",
        "import random\n",
        "from functools import lru_cache\n",
        "from tqdm import tqdm\n",
        "import zipfile\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from transformers import ElectraTokenizerFast, ElectraForSequenceClassification, AutoModelForCausalLM, AutoTokenizer\n",
        "from google.colab import files\n",
        "from tqdm.notebook import tqdm\n",
        "import os\n",
        "import json\n",
        "from google.colab import files\n",
        "# Set up device and logging\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {DEVICE}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Optimize TensorFlow and PyTorch operations\n",
        "os.environ[\"TF_ENABLE_ONEDNN_OPTS\"] = \"1\"\n",
        "os.environ[\"TF_GPU_THREAD_MODE\"] = \"gpu_private\"\n",
        "os.environ[\"TF_GPU_THREAD_COUNT\"] = \"4\"\n",
        "\n",
        "# For PyTorch\n",
        "torch.backends.cudnn.benchmark = True\n",
        "torch.set_float32_matmul_precision('high')"
      ],
      "metadata": {
        "id": "gGbwpbMF5lbR"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HDVS2hTwDcwg"
      },
      "source": [
        "#EntailmentModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IOm4n6WiGCbQ",
        "outputId": "3ace8835-0f41-4a2b-bef2-e5968c3858d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Extraction complete.\n"
          ]
        }
      ],
      "source": [
        "#ues our trained fine-tunied model\n",
        "#get the finetuned model from drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "zip_path = \"/content/drive/My Drive/araelectra-nli-finetuned.zip\"  # Adjust the path if needed\n",
        "extract_path = \"/content/araelectra\"\n",
        "\n",
        "# Extract the zip file\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "print(\"Extraction complete.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "BtaTMvXqCvS3"
      },
      "outputs": [],
      "source": [
        "class ArabicEntailmentModel:\n",
        "    \"\"\"Arabic entailment checker using AraELECTRA model.\"\"\"\n",
        "    def __init__(self, model_path=\"/content/araelectra/araelectra-nli-finetuned\"):\n",
        "        \"\"\"Initialize the model with better caching.\"\"\"\n",
        "        print(\"Loading AraELECTRA model for Arabic entailment checking...\")\n",
        "        self.tokenizer = ElectraTokenizerFast.from_pretrained(model_path)\n",
        "        self.model = ElectraForSequenceClassification.from_pretrained(model_path)\n",
        "        self.model = self.model.to(DEVICE)\n",
        "        self.model.eval()  # Set to evaluation mode\n",
        "\n",
        "        # More efficient cache implementation\n",
        "        self.cache_file = \"entailment_cache.pkl\"\n",
        "        self.cache = {}\n",
        "        self._load_cache()\n",
        "\n",
        "        # Add batch processing capabilities\n",
        "        self.batch_size = 16\n",
        "        print(\"AraELECTRA model loaded successfully\")\n",
        "\n",
        "    def _load_cache(self):\n",
        "        try:\n",
        "            if os.path.exists(self.cache_file):\n",
        "                with open(self.cache_file, 'rb') as f:\n",
        "                    self.cache = pickle.load(f)\n",
        "                print(f\"Loaded {len(self.cache)} cached entailment results\")\n",
        "        except Exception as e:\n",
        "            print(f\"Cache loading failed: {e}\")\n",
        "            self.cache = {}\n",
        "\n",
        "    def _save_cache(self):\n",
        "        try:\n",
        "            with open(self.cache_file, 'wb') as f:\n",
        "                pickle.dump(self.cache, f)\n",
        "        except Exception as e:\n",
        "            print(f\"Cache saving failed: {e}\")\n",
        "\n",
        "    def check_implications_batch(self, text_pairs):\n",
        "        \"\"\"Process multiple text pairs in one batch.\"\"\"\n",
        "        results = []\n",
        "        uncached_pairs = []\n",
        "        uncached_indices = []\n",
        "\n",
        "        # Check cache first\n",
        "        for i, (text1, text2) in enumerate(text_pairs):\n",
        "            cache_key = (text1, text2)\n",
        "            if cache_key in self.cache:\n",
        "                results.append(self.cache[cache_key])\n",
        "            else:\n",
        "                uncached_pairs.append((text1, text2))\n",
        "                uncached_indices.append(i)\n",
        "\n",
        "        # Process uncached pairs in batches\n",
        "        if uncached_pairs:\n",
        "            batch_results = []\n",
        "            for i in range(0, len(uncached_pairs), self.batch_size):\n",
        "                batch = uncached_pairs[i:i+self.batch_size]\n",
        "                batch_inputs = []\n",
        "\n",
        "                for text1, text2 in batch:\n",
        "                    encoded = self.tokenizer(\n",
        "                        text1,\n",
        "                        text2,\n",
        "                        padding=True,\n",
        "                        truncation=True,\n",
        "                        max_length=128,\n",
        "                        return_tensors=\"pt\"\n",
        "                    )\n",
        "                    batch_inputs.append({k: v.unsqueeze(0) for k, v in encoded.items()})\n",
        "\n",
        "                # Concatenate all inputs into one batch tensor\n",
        "                batch_tensors = {\n",
        "                    k: torch.cat([inp[k] for inp in batch_inputs], dim=0).to(DEVICE)\n",
        "                    for k in batch_inputs[0].keys()\n",
        "                }\n",
        "\n",
        "                # Process the batch\n",
        "                with torch.no_grad():\n",
        "                    outputs = self.model(**batch_tensors)\n",
        "                    logits = outputs.logits\n",
        "                    probs = F.softmax(logits, dim=1)\n",
        "                    predictions = torch.argmax(probs, dim=1).tolist()\n",
        "\n",
        "                # Convert to correct format and cache results\n",
        "                for j, pred in enumerate(predictions):\n",
        "                    text1, text2 = batch[j]\n",
        "                    result_map = {0: 2, 1: 1, 2: 0}\n",
        "                    result = result_map[pred]\n",
        "                    self.cache[(text1, text2)] = result\n",
        "                    batch_results.append(result)\n",
        "\n",
        "            # Insert batch results back into the right positions\n",
        "            for i, index in enumerate(uncached_indices):\n",
        "                results.insert(index, batch_results[i])\n",
        "\n",
        "            # Save cache after processing batch\n",
        "            if len(self.cache) % 100 == 0:\n",
        "                self._save_cache()\n",
        "\n",
        "        return results\n",
        "\n",
        "    def check_implication(self, text1, text2, example=None):\n",
        "        \"\"\"Check entailment for a single pair (backward compatible).\"\"\"\n",
        "        cache_key = (text1, text2)\n",
        "        if cache_key in self.cache:\n",
        "            return self.cache[cache_key]\n",
        "\n",
        "        # Prepare input\n",
        "        inputs = self.tokenizer(\n",
        "            text1,\n",
        "            text2,\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=128,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        # Move inputs to device\n",
        "        inputs = {k: v.to(DEVICE) for k, v in inputs.items()}\n",
        "\n",
        "        # Get prediction\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(**inputs)\n",
        "            logits = outputs.logits\n",
        "            probs = F.softmax(logits, dim=1)\n",
        "            predicted_class = torch.argmax(probs, dim=1).item()\n",
        "\n",
        "        # Map prediction\n",
        "        result_map = {0: 2, 1: 1, 2: 0}\n",
        "        result = result_map[predicted_class]\n",
        "\n",
        "        # Cache the result\n",
        "        self.cache[cache_key] = result\n",
        "\n",
        "        return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ulsao1Uw5jKm"
      },
      "source": [
        "# Clustering\n",
        "\n",
        "#also from their github got this\n",
        "\n",
        "```\n",
        "def get_semantic_ids(strings_list, model, strict_entailment=False, example=None):\n",
        "    \"\"\"Group list of predictions into semantic meaning.\"\"\"\n",
        "\n",
        "    def are_equivalent(text1, text2):\n",
        "\n",
        "        implication_1 = model.check_implication(text1, text2, example=example)\n",
        "        implication_2 = model.check_implication(text2, text1, example=example)  # pylint: disable=arguments-out-of-order\n",
        "        assert (implication_1 in [0, 1, 2]) and (implication_2 in [0, 1, 2])\n",
        "\n",
        "        if strict_entailment:\n",
        "            semantically_equivalent = (implication_1 == 2) and (implication_2 == 2)\n",
        "\n",
        "        else:\n",
        "            implications = [implication_1, implication_2]\n",
        "            # Check if none of the implications are 0 (contradiction) and not both of them are neutral.\n",
        "            semantically_equivalent = (0 not in implications) and ([1, 1] != implications)\n",
        "\n",
        "        return semantically_equivalent\n",
        "\n",
        "    # Initialise all ids with -1.\n",
        "    semantic_set_ids = [-1] * len(strings_list)\n",
        "    # Keep track of current id.\n",
        "    next_id = 0\n",
        "    for i, string1 in enumerate(strings_list):\n",
        "        # Check if string1 already has an id assigned.\n",
        "        if semantic_set_ids[i] == -1:\n",
        "            # If string1 has not been assigned an id, assign it next_id.\n",
        "            semantic_set_ids[i] = next_id\n",
        "            for j in range(i+1, len(strings_list)):\n",
        "                # Search through all remaining strings. If they are equivalent to string1, assign them the same id.\n",
        "                if are_equivalent(string1, strings_list[j]):\n",
        "                    semantic_set_ids[j] = next_id\n",
        "            next_id += 1\n",
        "\n",
        "    assert -1 not in semantic_set_ids\n",
        "\n",
        "    return semantic_set_ids\n",
        "\n",
        "\n",
        "def logsumexp_by_id(semantic_ids, log_likelihoods, agg='sum_normalized'):\n",
        "    \"\"\"Sum probabilities with the same semantic id.\n",
        "\n",
        "    Log-Sum-Exp because input and output probabilities in log space.\n",
        "    \"\"\"\n",
        "    unique_ids = sorted(list(set(semantic_ids)))\n",
        "    assert unique_ids == list(range(len(unique_ids)))\n",
        "    log_likelihood_per_semantic_id = []\n",
        "\n",
        "    for uid in unique_ids:\n",
        "        # Find positions in `semantic_ids` which belong to the active `uid`.\n",
        "        id_indices = [pos for pos, x in enumerate(semantic_ids) if x == uid]\n",
        "        # Gather log likelihoods at these indices.\n",
        "        id_log_likelihoods = [log_likelihoods[i] for i in id_indices]\n",
        "        if agg == 'sum_normalized':\n",
        "            # log_lik_norm = id_log_likelihoods - np.prod(log_likelihoods)\n",
        "            log_lik_norm = id_log_likelihoods - np.log(np.sum(np.exp(log_likelihoods)))\n",
        "            logsumexp_value = np.log(np.sum(np.exp(log_lik_norm)))\n",
        "        else:\n",
        "            raise ValueError\n",
        "        log_likelihood_per_semantic_id.append(logsumexp_value)\n",
        "\n",
        "    return log_likelihood_per_semantic_id\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "NW3ag2a4LHhN"
      },
      "outputs": [],
      "source": [
        "def get_semantic_ids(strings_list, model, strict_entailment=False, example=None):\n",
        "\n",
        "    # Group list of predictions into semantic meaning\n",
        "\n",
        "    @lru_cache(maxsize=None)\n",
        "    def are_equivalent(text1, text2):\n",
        "        # Check if text1 entails text2\n",
        "        implication_1 = model.check_implication(text1, text2, example=example)\n",
        "        # Check if text2 entails text1\n",
        "        implication_2 = model.check_implication(text2, text1, example=example)\n",
        "        assert (implication_1 in [0, 1, 2]) and (implication_2 in [0, 1, 2])\n",
        "\n",
        "        if strict_entailment:\n",
        "            # Both must indicate entailment (2) for semantic equivalence\n",
        "            semantically_equivalent = (implication_1 == 2) and (implication_2 == 2)\n",
        "        else:\n",
        "            implications = [implication_1, implication_2]\n",
        "            # Check if none of the implications are 0 (contradiction) and not both of them are neutral.)\n",
        "            semantically_equivalent = (0 not in implications) and ([1, 1] != implications)\n",
        "\n",
        "        return semantically_equivalent\n",
        "\n",
        "    # Initialize all ids with -1\n",
        "    semantic_set_ids = [-1] * len(strings_list)\n",
        "    # Keep track of current id\n",
        "    next_id = 0\n",
        "\n",
        "    for i, string1 in enumerate(strings_list):\n",
        "        # Check if string1 already has an id assigned\n",
        "        if semantic_set_ids[i] == -1:\n",
        "            # If string1 has not been assigned an id, assign it next_id\n",
        "            semantic_set_ids[i] = next_id\n",
        "            for j in range(i+1, len(strings_list)):\n",
        "                # Search through all remaining strings. If they are equivalent to string1, assign them the same id.\n",
        "                if semantic_set_ids[j] == -1 and are_equivalent(string1, strings_list[j]):\n",
        "                    semantic_set_ids[j] = next_id\n",
        "            next_id += 1\n",
        "\n",
        "    assert -1 not in semantic_set_ids\n",
        "\n",
        "    return semantic_set_ids\n",
        "\n",
        "def logsumexp_by_id(semantic_ids, log_likelihoods, agg='sum_normalized'):\n",
        "    \"\"\"\n",
        "    Sum probabilities with the same semantic ID.\n",
        "    Log-Sum-Exp because input and output probabilities in log space.\n",
        "    \"\"\"\n",
        "    unique_ids = sorted(list(set(semantic_ids)))\n",
        "    assert unique_ids == list(range(len(unique_ids)))\n",
        "    log_likelihood_per_semantic_id = []\n",
        "\n",
        "    for uid in unique_ids:\n",
        "        # Find positions in `semantic_ids` which belong to the active `uid`\n",
        "        id_indices = [pos for pos, x in enumerate(semantic_ids) if x == uid]\n",
        "        # Gather log likelihoods at these indices\n",
        "        id_log_likelihoods = [log_likelihoods[i] for i in id_indices]\n",
        "\n",
        "        if agg == 'sum_normalized':\n",
        "            # Normalize by subtracting the log sum exp of all log likelihoods\n",
        "            # log_lik_norm = id_log_likelihoods - np.prod(log_likelihoods)\n",
        "            log_lik_norm = id_log_likelihoods - np.log(np.sum(np.exp(log_likelihoods)))\n",
        "            logsumexp_value = np.log(np.sum(np.exp(log_lik_norm)))\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown aggregation method: {agg}\")\n",
        "\n",
        "        log_likelihood_per_semantic_id.append(logsumexp_value)\n",
        "\n",
        "    return log_likelihood_per_semantic_id"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2wHskqM9lkQ"
      },
      "source": [
        "#entropy\n",
        "\n",
        "#from their github i got this code\n",
        "\n",
        "```\n",
        "\n",
        "def predictive_entropy(log_probs):\n",
        "    \"\"\"Compute MC estimate of entropy.\n",
        "\n",
        "    `E[-log p(x)] ~= -1/N sum_i log p(x_i)`, i.e. the average token likelihood.\n",
        "    \"\"\"\n",
        "\n",
        "    entropy = -np.sum(log_probs) / len(log_probs)\n",
        "\n",
        "    return entropy\n",
        "\n",
        "\n",
        "def predictive_entropy_rao(log_probs):\n",
        "    entropy = -np.sum(np.exp(log_probs) * log_probs)\n",
        "    return entropy\n",
        "\n",
        "\n",
        "def cluster_assignment_entropy(semantic_ids):\n",
        "    \"\"\"Estimate semantic uncertainty from how often different clusters get assigned.\n",
        "\n",
        "    We estimate the categorical distribution over cluster assignments from the\n",
        "    semantic ids. The uncertainty is then given by the entropy of that\n",
        "    distribution. This estimate does not use token likelihoods, it relies soley\n",
        "    on the cluster assignments. If probability mass is spread of between many\n",
        "    clusters, entropy is larger. If probability mass is concentrated on a few\n",
        "    clusters, entropy is small.\n",
        "\n",
        "    Input:\n",
        "        semantic_ids: List of semantic ids, e.g. [0, 1, 2, 1].\n",
        "    Output:\n",
        "        cluster_entropy: Entropy, e.g. (-p log p).sum() for p = [1/4, 2/4, 1/4].\n",
        "    \"\"\"\n",
        "\n",
        "    n_generations = len(semantic_ids)\n",
        "    counts = np.bincount(semantic_ids)\n",
        "    probabilities = counts/n_generations\n",
        "    assert np.isclose(probabilities.sum(), 1)\n",
        "    entropy = - (probabilities * np.log(probabilities)).sum()\n",
        "    return entropy\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Euc93oDFGKCa"
      },
      "outputs": [],
      "source": [
        "def predictive_entropy_rao(log_probs):\n",
        "    \"\"\"\n",
        "    Compute entropy from log probabilities.\n",
        "\n",
        "    Parameters:\n",
        "    - log_probs: Log probabilities\n",
        "\n",
        "    Returns:\n",
        "    - Entropy value\n",
        "    \"\"\"\n",
        "    entropy = -np.sum(np.exp(log_probs) * log_probs)\n",
        "    return entropy\n",
        "\n",
        "def predictive_entropy(log_probs):\n",
        "    \"\"\"Compute MC estimate of entropy.\n",
        "\n",
        "    `E[-log p(x)] ~= -1/N sum_i log p(x_i)`, i.e. the average token likelihood.\n",
        "    \"\"\"\n",
        "\n",
        "    entropy = -np.sum(log_probs) / len(log_probs)\n",
        "\n",
        "    return entropy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnKq6Ixs9iSr"
      },
      "source": [
        "# call function\n",
        "The actual full computation in the original repo happens in compute_uncertainty_measures.py:\n",
        " This  code is where they call the functions to compute the entropy measures\n",
        "\n",
        "```\n",
        "\n",
        "if args.compute_predictive_entropy:\n",
        "    # Token log likelihoods. Shape = (n_sample, n_tokens)\n",
        "    if not args.use_all_generations:\n",
        "        log_liks = [r[1] for r in full_responses[:args.use_num_generations]]\n",
        "    else:\n",
        "        log_liks = [r[1] for r in full_responses]\n",
        "\n",
        "    for i in log_liks:\n",
        "        assert i\n",
        "\n",
        "    if args.compute_context_entails_response:\n",
        "        # Compute context entails answer baseline.\n",
        "        entropies['context_entails_response'].append(context_entails_response(\n",
        "            context, responses, entailment_model))\n",
        "\n",
        "    if args.condition_on_question and args.entailment_model == 'deberta':\n",
        "        responses = [f'{question} {r}' for r in responses]\n",
        "\n",
        "    # Compute semantic ids.\n",
        "    semantic_ids = get_semantic_ids(\n",
        "        responses, model=entailment_model,\n",
        "        strict_entailment=args.strict_entailment, example=example)\n",
        "\n",
        "    result_dict['semantic_ids'].append(semantic_ids)\n",
        "\n",
        "    # Compute entropy from frequencies of cluster assignments.\n",
        "    entropies['cluster_assignment_entropy'].append(cluster_assignment_entropy(semantic_ids))\n",
        "\n",
        "    # Length normalization of generation probabilities.\n",
        "    log_liks_agg = [np.mean(log_lik) for log_lik in log_liks]\n",
        "\n",
        "    # Compute naive entropy.\n",
        "    entropies['regular_entropy'].append(predictive_entropy(log_liks_agg))\n",
        "\n",
        "    # Compute semantic entropy.\n",
        "    log_likelihood_per_semantic_id = logsumexp_by_id(semantic_ids, log_liks_agg, agg='sum_normalized')\n",
        "    pe = predictive_entropy_rao(log_likelihood_per_semantic_id)\n",
        "    entropies['semantic_entropy'].append(pe)\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "_uy0aCxkIL_R"
      },
      "outputs": [],
      "source": [
        "def experiment_semantic_entropy(question, original_answer ,llm_model, llm_tokenizer, entailment_model, num_samples):\n",
        "    \"\"\"\n",
        "    Compute semantic entropy for a given question.\n",
        "    Returns:\n",
        "        Dictionary containing results of the experiment\n",
        "    \"\"\"\n",
        "    # Step 1: Generate multiple answers with their token log likelihoods\n",
        "    #print(f\"Generating {num_samples} answers for question: {question}\")\n",
        "    results = generate_answer(question, num_samples, llm_model, llm_tokenizer)\n",
        "\n",
        "    # Step 2: Extract answers and their log likelihoods\n",
        "    answers = [result['text'] for result in results]\n",
        "    # length normalization of the log probabilities\n",
        "    log_likelihoods = [np.mean(result['token_log_probs']) for result in results]\n",
        "\n",
        "   # print(f\"Generated {len(answers)} answers\")\n",
        "\n",
        "    # Step 3: Create an example dictionary for entailment checking\n",
        "    example = {'question': question}\n",
        "\n",
        "    # Step 4: Compute semantic clusters\n",
        "   # print(\"Computing semantic clusters...\")\n",
        "    semantic_ids = get_semantic_ids(answers, entailment_model, strict_entailment=False, example=example)\n",
        "    unique_clusters = len(set(semantic_ids))\n",
        "   # print(f\"Found {unique_clusters} semantic clusters\")\n",
        "\n",
        "    # Step 5: Calculate entropy measures\n",
        "    # naive_entropy calculation (based on log likelihoods only)\n",
        "    naive_entropy = predictive_entropy(log_likelihoods)\n",
        "\n",
        "    # Semantic entropy calculation (based on semantic clusters and log likelihoods)\n",
        "    log_likelihood_per_semantic_id = logsumexp_by_id(semantic_ids, log_likelihoods, agg='sum_normalized')\n",
        "    semantic_entropy = predictive_entropy_rao(log_likelihood_per_semantic_id)\n",
        "\n",
        "    # Step 6: Print results\n",
        "    print(f\"\\nEntropy Analysis for: '{question}'\")\n",
        "    print(f\"Generated {len(answers)} answers in {unique_clusters} semantic clusters\")\n",
        "    #print the entropy values with 4 decimal places\n",
        "    print(f\" naive Entropy: {naive_entropy:.4f}\")\n",
        "    print(f\"Semantic Entropy: {semantic_entropy:.4f}\")\n",
        "\n",
        "\n",
        "    # Step 7: Display cluster information\n",
        "    #print(\"\\nSemantic Clusters:\")\n",
        "    unique_clusters_list = sorted(list(set(semantic_ids)))\n",
        "    for cluster_id in unique_clusters_list:\n",
        "        cluster_items = [answers[i] for i, sid in enumerate(semantic_ids) if sid == cluster_id]\n",
        "        count = len(cluster_items)\n",
        "        '''\n",
        "        print(f\"\\nCluster {cluster_id} ({count} items):\")\n",
        "        for item in cluster_items:\n",
        "            print(f\"  - {item}\")\n",
        "        '''\n",
        "    # Format answers in the  structure\n",
        "    formatted_answers = []\n",
        "    for cluster_id in unique_clusters_list:\n",
        "        cluster_answers = [answers[i] for i, sid in enumerate(semantic_ids) if sid == cluster_id]\n",
        "        formatted_answers.append(cluster_answers)\n",
        "\n",
        "    # Return  results\n",
        "    # number like 2.220446049250313e-16 is extremely close to zero (basically floating-point noise).\n",
        "    # so rounds the number to 4 decimal places\n",
        "  # Return results with details and formatted answers\n",
        "    return {\n",
        "        'question': question,\n",
        "        'original_answer': original_answer,  # Include the original answer from the dataset\n",
        "        'answers': formatted_answers,  # Answers formatted\n",
        "        'log_likelihoods': log_likelihoods,\n",
        "        'semantic_ids': semantic_ids,\n",
        "        'naive_entropy': round(naive_entropy, 4),\n",
        "        'semantic_entropy': round(semantic_entropy, 4),\n",
        "        'num_clusters': unique_clusters,\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0EoVG07fDNN"
      },
      "source": [
        "#get likelihood\n",
        "\n",
        "#from their code\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "#at this function\n",
        "def predict(self, input_data, temperature, return_full=False):\n",
        "\n",
        "\n",
        "        # Get log_likelihoods.\n",
        "        # outputs.scores are the logits for the generated token.\n",
        "        # outputs.scores is a tuple of len = n_generated_tokens.\n",
        "        # Each entry is shape (bs, vocabulary size).\n",
        "        # outputs.sequences is the sequence of all tokens: input and generated.\n",
        "        transition_scores = self.model.compute_transition_scores(\n",
        "            outputs.sequences, outputs.scores, normalize_logits=True)\n",
        "        # Transition_scores[0] only contains the scores for the first generated tokens.\n",
        "\n",
        "        log_likelihoods = [score.item() for score in transition_scores[0]]\n",
        "        if len(log_likelihoods) == 1:\n",
        "            logging.warning('Taking first and only generation for log likelihood!')\n",
        "            log_likelihoods = log_likelihoods\n",
        "        else:\n",
        "            log_likelihoods = log_likelihoods[:n_generated]\n",
        "\n",
        "        if len(log_likelihoods) == self.max_new_tokens:\n",
        "            logging.warning('Generation interrupted by max_token limit.')\n",
        "\n",
        "        if len(log_likelihoods) == 0:\n",
        "            raise ValueError\n",
        "\n",
        "        return sliced_answer, log_likelihoods, last_token_embedding\n",
        "\n",
        "\n",
        "all function if needed\n",
        "f predict(self, input_data, temperature, return_full=False):\n",
        "\n",
        "        # Implement prediction.\n",
        "        inputs = self.tokenizer(input_data, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "        if 'llama' in self.model_name.lower() or 'falcon' in self.model_name or 'mistral' in self.model_name.lower():\n",
        "            if 'token_type_ids' in inputs:  # Some HF models have changed.\n",
        "                del inputs['token_type_ids']\n",
        "            pad_token_id = self.tokenizer.eos_token_id\n",
        "        else:\n",
        "            pad_token_id = None\n",
        "\n",
        "        if self.stop_sequences is not None:\n",
        "            stopping_criteria = StoppingCriteriaList([StoppingCriteriaSub(\n",
        "                stops=self.stop_sequences,\n",
        "                initial_length=len(inputs['input_ids'][0]),\n",
        "                tokenizer=self.tokenizer)])\n",
        "        else:\n",
        "            stopping_criteria = None\n",
        "\n",
        "        logging.debug('temperature: %f', temperature)\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=self.max_new_tokens,\n",
        "                return_dict_in_generate=True,\n",
        "                output_scores=True,\n",
        "                output_hidden_states=True,\n",
        "                temperature=temperature,\n",
        "                do_sample=True,\n",
        "                stopping_criteria=stopping_criteria,\n",
        "                pad_token_id=pad_token_id,\n",
        "            )\n",
        "\n",
        "        if len(outputs.sequences[0]) > self.token_limit:\n",
        "            raise ValueError(\n",
        "                'Generation exceeding token limit %d > %d',\n",
        "                len(outputs.sequences[0]), self.token_limit)\n",
        "\n",
        "        full_answer = self.tokenizer.decode(\n",
        "            outputs.sequences[0], skip_special_tokens=True)\n",
        "\n",
        "        if return_full:\n",
        "            return full_answer\n",
        "\n",
        "        # For some models, we need to remove the input_data from the answer.\n",
        "        if full_answer.startswith(input_data):\n",
        "            input_data_offset = len(input_data)\n",
        "        else:\n",
        "            raise ValueError('Have not tested this in a while.')\n",
        "\n",
        "        # Remove input from answer.\n",
        "        answer = full_answer[input_data_offset:]\n",
        "\n",
        "        # Remove stop_words from answer.\n",
        "        stop_at = len(answer)\n",
        "        sliced_answer = answer\n",
        "        if self.stop_sequences is not None:\n",
        "            for stop in self.stop_sequences:\n",
        "                if answer.endswith(stop):\n",
        "                    stop_at = len(answer) - len(stop)\n",
        "                    sliced_answer = answer[:stop_at]\n",
        "                    break\n",
        "            if not all([stop not in sliced_answer for stop in self.stop_sequences]):\n",
        "                error_msg = 'Error: Stop words not removed successfully!'\n",
        "                error_msg += f'Answer: >{answer}< '\n",
        "                error_msg += f'Sliced Answer: >{sliced_answer}<'\n",
        "                if 'falcon' not in self.model_name.lower():\n",
        "                    raise ValueError(error_msg)\n",
        "                else:\n",
        "                    logging.error(error_msg)\n",
        "\n",
        "        # Remove whitespaces from answer (in particular from beginning.)\n",
        "        sliced_answer = sliced_answer.strip()\n",
        "\n",
        "        # Get the number of tokens until the stop word comes up.\n",
        "        # Note: Indexing with `stop_at` already excludes the stop_token.\n",
        "        # Note: It's important we do this with full answer, since there might be\n",
        "        # non-trivial interactions between the input_data and generated part\n",
        "        # in tokenization (particularly around whitespaces.)\n",
        "        token_stop_index = self.tokenizer(full_answer[:input_data_offset + stop_at], return_tensors=\"pt\")['input_ids'].shape[1]\n",
        "        n_input_token = len(inputs['input_ids'][0])\n",
        "        n_generated = token_stop_index - n_input_token\n",
        "\n",
        "        if n_generated == 0:\n",
        "            logging.warning('Only stop_words were generated. For likelihoods and embeddings, taking stop word instead.')\n",
        "            n_generated = 1\n",
        "\n",
        "        # Get the last hidden state (last layer) and the last token's embedding of the answer.\n",
        "        # Note: We do not want this to be the stop token.\n",
        "\n",
        "        # outputs.hidden_state is a tuple of len = n_generated_tokens.\n",
        "        # The first hidden state is for the input tokens and is of shape\n",
        "        #     (n_layers) x (batch_size, input_size, hidden_size).\n",
        "        # (Note this includes the first generated token!)\n",
        "        # The remaining hidden states are for the remaining generated tokens and is of shape\n",
        "        #    (n_layers) x (batch_size, 1, hidden_size).\n",
        "\n",
        "        # Note: The output embeddings have the shape (batch_size, generated_length, hidden_size).\n",
        "        # We do not get embeddings for input_data! We thus subtract the n_tokens_in_input from\n",
        "        # token_stop_index to arrive at the right output.\n",
        "\n",
        "        if 'decoder_hidden_states' in outputs.keys():\n",
        "            hidden = outputs.decoder_hidden_states\n",
        "        else:\n",
        "            hidden = outputs.hidden_states\n",
        "\n",
        "        if len(hidden) == 1:\n",
        "            logging.warning(\n",
        "                'Taking first and only generation for hidden! '\n",
        "                'n_generated: %d, n_input_token: %d, token_stop_index %d, '\n",
        "                'last_token: %s, generation was: %s',\n",
        "                n_generated, n_input_token, token_stop_index,\n",
        "                self.tokenizer.decode(outputs['sequences'][0][-1]),\n",
        "                full_answer,\n",
        "                )\n",
        "            last_input = hidden[0]\n",
        "        elif ((n_generated - 1) >= len(hidden)):\n",
        "            # If access idx is larger/equal.\n",
        "            logging.error(\n",
        "                'Taking last state because n_generated is too large'\n",
        "                'n_generated: %d, n_input_token: %d, token_stop_index %d, '\n",
        "                'last_token: %s, generation was: %s, slice_answer: %s',\n",
        "                n_generated, n_input_token, token_stop_index,\n",
        "                self.tokenizer.decode(outputs['sequences'][0][-1]),\n",
        "                full_answer, sliced_answer\n",
        "                )\n",
        "            last_input = hidden[-1]\n",
        "        else:\n",
        "            last_input = hidden[n_generated - 1]\n",
        "\n",
        "        # Then access last layer for input\n",
        "        last_layer = last_input[-1]\n",
        "        # Then access last token in input.\n",
        "        last_token_embedding = last_layer[:, -1, :].cpu()\n",
        "\n",
        "        # Get log_likelihoods.\n",
        "        # outputs.scores are the logits for the generated token.\n",
        "        # outputs.scores is a tuple of len = n_generated_tokens.\n",
        "        # Each entry is shape (bs, vocabulary size).\n",
        "        # outputs.sequences is the sequence of all tokens: input and generated.\n",
        "        transition_scores = self.model.compute_transition_scores(\n",
        "            outputs.sequences, outputs.scores, normalize_logits=True)\n",
        "        # Transition_scores[0] only contains the scores for the first generated tokens.\n",
        "\n",
        "        log_likelihoods = [score.item() for score in transition_scores[0]]\n",
        "        if len(log_likelihoods) == 1:\n",
        "            logging.warning('Taking first and only generation for log likelihood!')\n",
        "            log_likelihoods = log_likelihoods\n",
        "        else:\n",
        "            log_likelihoods = log_likelihoods[:n_generated]\n",
        "\n",
        "        if len(log_likelihoods) == self.max_new_tokens:\n",
        "            logging.warning('Generation interrupted by max_token limit.')\n",
        "\n",
        "        if len(log_likelihoods) == 0:\n",
        "            raise ValueError\n",
        "\n",
        "        return sliced_answer, log_likelihoods, last_token_embedding\n",
        "\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFYlG7k_gCIS"
      },
      "source": [
        "##log probabilities vs negative log likelihoods:\n",
        "\n",
        "- Log probabilities are the logarithm of the probability: log(p)\n",
        "- Negative log likelihoods are the negative logarithm of the probability: -log(p)\n",
        "\n",
        "The original code is working with log probabilities, not negative log likelihoods. We see normalize_logits=True in compute_transition_scores, it means the model is returning log probabilities.\n",
        "\n",
        "\n",
        "**The original code is using predictive_entropy_rao, which expects log probabilities as input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "JXUkp0AdvHBb"
      },
      "outputs": [],
      "source": [
        "def generate_answer(question, num_samples, model, tokenizer):\n",
        "    \"\"\"\n",
        "    Generate multiple answers to a question using LLM with direct token log likelihoods.\n",
        "    \"\"\"\n",
        "    # Create the prompt with the question\n",
        "    prompt = f\"أجب على السؤال التالي بجملة واحدة فقط موجزة ولكن كاملة باللغة العربية\\nQuestion: {question}\\nAnswer:\"\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for _ in range(num_samples):\n",
        "        # Tokenize the prompt\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "        prompt_length = inputs.input_ids.shape[1]  # Number of tokens in the prompt\n",
        "\n",
        "        # Generate with return_dict_in_generate=True and output_scores=True to get scores\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                inputs.input_ids,\n",
        "                max_new_tokens=100,\n",
        "                do_sample=True,\n",
        "                temperature=0.5,\n",
        "                return_dict_in_generate=True,\n",
        "                output_scores=True,\n",
        "            )\n",
        "\n",
        "        # Calculate token log probabilities using compute_transition_scores\n",
        "        # normalize_logits=True ensures we get log probabilities\n",
        "        transition_scores = model.compute_transition_scores(\n",
        "            outputs.sequences,\n",
        "            outputs.scores,\n",
        "            normalize_logits=True\n",
        "        )\n",
        "\n",
        "        # Extract log likelihoods like they did exactly, but no handeling of cases\n",
        "        log_likelihoods = [score.item() for score in transition_scores[0]]\n",
        "\n",
        "\n",
        "        # Decode the generated text\n",
        "        generated_text = tokenizer.decode(outputs.sequences[0], skip_special_tokens=True)\n",
        "        answer = generated_text.split(\"Answer:\")[-1].strip()\n",
        "\n",
        "        # Clean the output\n",
        "        strings_to_filter_on = ['.', '\\n', 'Q:', 'A:', 'question:', 'answer:', 'Question:', 'Answer:',\n",
        "                               'Questions:', 'questions:', 'QUESTION:', 'ANSWER:']\n",
        "        for string in strings_to_filter_on:\n",
        "            if string in answer:\n",
        "                answer = answer.split(string)[0].strip()\n",
        "\n",
        "        results.append({\n",
        "            'text': answer,\n",
        "            'token_log_probs': log_likelihoods,  # Store raw log probabilities\n",
        "\n",
        "        })\n",
        "\n",
        "    return results # Return results AFTER the loop completes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKCz4R7jnK7D"
      },
      "source": [
        "# Main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "a8ag496J_opE"
      },
      "outputs": [],
      "source": [
        "def load_qa_dataset(dataset_name, file_path=None):\n",
        "    data = []\n",
        "\n",
        "    try:\n",
        "        if dataset_name == 'arabicaqa' and file_path:\n",
        "            if os.path.exists(file_path):  # Use the loaded file\n",
        "                print(\"Using ArabicaQA\")\n",
        "                with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                    custom_data = json.load(f)\n",
        "                    for idx, item in enumerate(custom_data):\n",
        "                        data.append({\n",
        "                            \"question_id\": idx,\n",
        "                            \"Question\": item[\"question\"],\n",
        "                            \"Answer\": item[\"answer\"]\n",
        "                        })\n",
        "            else:\n",
        "                raise FileNotFoundError(f\"ArabicaQA file not found at {file_path}\")\n",
        "\n",
        "        elif dataset_name == 'xor_tydiqa' and file_path:\n",
        "            if os.path.exists(file_path):  # Use the loaded file\n",
        "                print(\"Using XOR-TyDiQA\")\n",
        "                print(\"Filtering Arabic QA pairs from XOR-TyDi...\")\n",
        "\n",
        "                # Load the jsonl dataset\n",
        "                with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                    custom_data = [json.loads(line) for line in f]  # Handling jsonl\n",
        "\n",
        "                # Filter Arabic samples (\"lang\" = \"ar\")\n",
        "                arabic_data = []\n",
        "                for idx, item in enumerate(custom_data):\n",
        "                    if item[\"lang\"] == \"ar\":  # Arabic language code\n",
        "                        arabic_data.append({\n",
        "                            \"question_id\": item[\"id\"],\n",
        "                            \"Question\": item[\"question\"],\n",
        "                            \"Answer\": item[\"answers\"][0]  # First answer in list\n",
        "                        })\n",
        "\n",
        "                data.extend(arabic_data)\n",
        "            else:\n",
        "                raise FileNotFoundError(f\"XOR-TyDiQA file not found at {file_path}\")\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported dataset: {dataset_name}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading {dataset_name}: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "5B6fdiUCITVs",
        "outputId": "4ca4bf07-b68a-4819-8c43-6d1b1cd7042f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading language model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/1.64k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "03aedc5b7aeb4daba7045132d4787003"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/4.85M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5081bfc8a55b41bd90d8fb838f55d7c7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/131 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "206bce0a1efa44d48b645678f8e1ce7f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/1.32k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c127626a84734854aacd3cab6a2c027b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-5d11b17c9c1e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mmodel_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"inceptionai/jais-family-6p7b-chat\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mllm_tokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         llm_model = AutoModelForCausalLM.from_pretrained(\n\u001b[0m\u001b[1;32m     44\u001b[0m             \u001b[0mmodel_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0mtorch_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    529\u001b[0m                 \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"quantization_config\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m             config, kwargs = AutoConfig.from_pretrained(\n\u001b[0m\u001b[1;32m    532\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m                 \u001b[0mreturn_unused_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/configuration_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m   1115\u001b[0m         \u001b[0mhas_remote_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"auto_map\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"AutoConfig\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"auto_map\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m         \u001b[0mhas_local_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"model_type\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model_type\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mCONFIG_MAPPING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1117\u001b[0;31m         trust_remote_code = resolve_trust_remote_code(\n\u001b[0m\u001b[1;32m   1118\u001b[0m             \u001b[0mtrust_remote_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_local_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_remote_code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/dynamic_module_utils.py\u001b[0m in \u001b[0;36mresolve_trust_remote_code\u001b[0;34m(trust_remote_code, model_name, has_local_code, has_remote_code)\u001b[0m\n\u001b[1;32m    667\u001b[0m                 \u001b[0msignal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malarm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTIME_OUT_REMOTE_CODE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0mtrust_remote_code\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 669\u001b[0;31m                     answer = input(\n\u001b[0m\u001b[1;32m    670\u001b[0m                         \u001b[0;34mf\"The repository for {model_name} contains custom code which must be executed to correctly \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m                         \u001b[0;34mf\"load the model. You can inspect the repository content at https://hf.co/{model_name}.\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ],
      "source": [
        "# Function to safely load existing partial results\n",
        "def load_partial_results(partial_file_path):\n",
        "    if os.path.exists(partial_file_path):\n",
        "        with open(partial_file_path, 'r', encoding='utf-8') as f:\n",
        "            saved_results = json.load(f)\n",
        "        print(f\"✓ Loaded {len(saved_results)} saved results from checkpoint.\")\n",
        "        return saved_results\n",
        "    else:\n",
        "        print(\"✓ No previous checkpoint found. Starting fresh.\")\n",
        "        return []\n",
        "\n",
        "\n",
        "# Main\n",
        "if __name__ == \"__main__\":\n",
        "    MODEL_CHOICE = \"jais\"  # Options: \"llama\", \"allam\", \"jais\", \"qwen\"\n",
        "    START_FROM_QUESTION = 1602\n",
        "\n",
        "    print(\"Loading language model...\")\n",
        "\n",
        "    if MODEL_CHOICE == \"llama\":\n",
        "        model_id = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
        "        llm_model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_id,\n",
        "            torch_dtype=torch.float16,\n",
        "            device_map=\"auto\"\n",
        "        )\n",
        "        llm_tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "        print(f\"Loaded Llama 3.1 model successfully\")\n",
        "\n",
        "    elif MODEL_CHOICE == \"allam\":\n",
        "        model_id = \"ALLaM-AI/ALLaM-7B-Instruct-preview\"\n",
        "        llm_tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "        llm_model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_id,\n",
        "            torch_dtype=torch.float16,\n",
        "            device_map=\"auto\"\n",
        "        )\n",
        "        print(f\"Loaded ALLaM model successfully\")\n",
        "\n",
        "    elif MODEL_CHOICE == \"jais\":\n",
        "        model_id = \"inceptionai/jais-family-6p7b-chat\"\n",
        "        llm_tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "        llm_model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_id,\n",
        "            torch_dtype=torch.float16,\n",
        "            device_map=\"auto\"\n",
        "        )\n",
        "        print(f\"Loaded Jais model successfully\")\n",
        "\n",
        "    elif MODEL_CHOICE == \"qwen\":\n",
        "        model_id = \"Qwen/Qwen2-7B-Instruct\"\n",
        "        llm_tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "        llm_model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_id,\n",
        "            torch_dtype=torch.float16,\n",
        "            device_map=\"auto\"\n",
        "        )\n",
        "        print(f\"Loaded Qwen2 model successfully\")\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown model choice: {MODEL_CHOICE}\")\n",
        "\n",
        "    print(\"Loading entailment model...\")\n",
        "    entailment_model = ArabicEntailmentModel()\n",
        "    print(\"Entailment model loaded successfully\")\n",
        "\n",
        "    print(\"Loading dataset...\")\n",
        "    DATASET_CHOICE = 'arabicaqa'  # Options: 'arabicaqa', 'xor_tydiqa'\n",
        "    FILE_PATH = '/content/test-open.json'\n",
        "    data = load_qa_dataset(DATASET_CHOICE, FILE_PATH)\n",
        "    print(f\"Loaded {len(data)} questions from {DATASET_CHOICE}\")\n",
        "\n",
        "    print(\"\\nStarting semantic entropy experiments...\")\n",
        "\n",
        "    partial_save_path = f'semantic_entropy_{MODEL_CHOICE}_partial_results.json'\n",
        "\n",
        "    # Load previous partial results if available\n",
        "    results = load_partial_results(partial_save_path)\n",
        "\n",
        "    #  starting point\n",
        "    if START_FROM_QUESTION is not None:\n",
        "\n",
        "        already_processed = START_FROM_QUESTION\n",
        "        # Trim results to match the starting point if needed\n",
        "        if len(results) > already_processed:\n",
        "            results = results[:already_processed]\n",
        "            print(f\"Truncated results to match starting point at question {already_processed}\")\n",
        "    else:\n",
        "        # Use checkpoint\n",
        "        already_processed = len(results)\n",
        "\n",
        "    print(f\"Starting from question {already_processed + 1}\")\n",
        "\n",
        "    MAX_QUESTIONS = 7000  # the last question in a run\n",
        "    test_questions = data[:MAX_QUESTIONS] if MAX_QUESTIONS else data\n",
        "\n",
        "    for i, item in tqdm(enumerate(test_questions[already_processed:], start=already_processed),\n",
        "                         total=len(test_questions)-already_processed,\n",
        "                         desc=\"Processing questions\"):\n",
        "        question = item[\"Question\"]\n",
        "        original_answer = item[\"Answer\"]\n",
        "        print(f\"\\n[{i+1}/{len(test_questions)}] Processing question\")\n",
        "\n",
        "        result = experiment_semantic_entropy(\n",
        "            question=question,\n",
        "            original_answer=original_answer,\n",
        "            llm_model=llm_model,\n",
        "            llm_tokenizer=llm_tokenizer,\n",
        "            entailment_model=entailment_model,\n",
        "            num_samples=10\n",
        "        )\n",
        "        results.append(result)\n",
        "\n",
        "        # Save progress after every 5 questions\n",
        "        if (i+1) % 5 == 0:\n",
        "            with open(partial_save_path, 'w', encoding='utf-8') as f:\n",
        "                json.dump(results, f, ensure_ascii=False, indent=2)\n",
        "            print(f\"✓ Saved partial progress after {i+1} questions.\")\n",
        "\n",
        "    # Save final results\n",
        "    final_save_path = f'semantic_entropy_{MODEL_CHOICE}_{DATASET_CHOICE}_resultsP2.json'\n",
        "    print(\"\\nSaving final results...\")\n",
        "    with open(final_save_path, 'w', encoding='utf-8') as f:\n",
        "        json.dump(results, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "\n",
        "    files.download(final_save_path)\n",
        "    print(f\"✓ Experiment completed successfully with {MODEL_CHOICE} model!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to safely load existing partial results\n",
        "def load_partial_results(partial_file_path):\n",
        "    if os.path.exists(partial_file_path):\n",
        "        with open(partial_file_path, 'r', encoding='utf-8') as f:\n",
        "            saved_results = json.load(f)\n",
        "        print(f\"✓ Loaded {len(saved_results)} saved results from checkpoint.\")\n",
        "        return saved_results\n",
        "    else:\n",
        "        print(\"✓ No previous checkpoint found. Starting fresh.\")\n",
        "        return []\n",
        "\n",
        "\n",
        "# Main\n",
        "if __name__ == \"__main__\":\n",
        "    MODEL_CHOICE = \"jais\"  # Options: \"llama\", \"allam\", \"jais\", \"qwen\"\n",
        "\n",
        "    print(\"Loading language model...\")\n",
        "    if MODEL_CHOICE == \"llama\":\n",
        "        model_id = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
        "        llm_model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_id,\n",
        "            torch_dtype=torch.float16,\n",
        "            device_map=\"auto\"\n",
        "        )\n",
        "        llm_tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "        print(f\"Loaded Llama 3.1 model successfully\")\n",
        "\n",
        "    elif MODEL_CHOICE == \"allam\":\n",
        "        model_id = \"ALLaM-AI/ALLaM-7B-Instruct-preview\"\n",
        "        llm_tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "        llm_model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_id,\n",
        "            torch_dtype=torch.float16,\n",
        "            device_map=\"auto\"\n",
        "        )\n",
        "        print(f\"Loaded ALLaM model successfully\")\n",
        "\n",
        "    elif MODEL_CHOICE == \"jais\":\n",
        "        model_id = \"inceptionai/jais-family-6p7b-chat\"\n",
        "        llm_tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "        llm_model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_id,\n",
        "            torch_dtype=torch.float16,\n",
        "            device_map=\"auto\"\n",
        "        )\n",
        "        print(f\"Loaded Jais model successfully\")\n",
        "\n",
        "    elif MODEL_CHOICE == \"qwen\":\n",
        "        model_id = \"Qwen/Qwen2-7B-Instruct\"\n",
        "        llm_tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "        llm_model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_id,\n",
        "            torch_dtype=torch.float16,\n",
        "            device_map=\"auto\"\n",
        "        )\n",
        "        print(f\"Loaded Qwen2 model successfully\")\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown model choice: {MODEL_CHOICE}\")\n",
        "\n",
        "    print(\"Loading entailment model...\")\n",
        "    entailment_model = ArabicEntailmentModel()\n",
        "    print(\"Entailment model loaded successfully\")\n",
        "\n",
        "    print(\"Loading dataset...\")\n",
        "    DATASET_CHOICE = 'arabicaqa'  # Options: 'arabicaqa', 'xor_tydiqa'\n",
        "    FILE_PATH = '/content/test-open.json'\n",
        "    data = load_qa_dataset(DATASET_CHOICE, FILE_PATH)\n",
        "    print(f\"Loaded {len(data)} questions from {DATASET_CHOICE}\")\n",
        "\n",
        "    print(\"\\nStarting semantic entropy experiments...\")\n",
        "    partial_save_path = f'semantic_entropy_{MODEL_CHOICE}_partial_results.json'\n",
        "\n",
        "    # Load previous partial results if available\n",
        "    results = load_partial_results(partial_save_path)\n",
        "\n",
        "    already_processed = len(results)\n",
        "    print(f\"Starting from question {already_processed + 1}\")\n",
        "\n",
        "    MAX_QUESTIONS = 6000  # Set to a number or None\n",
        "    test_questions = data[:MAX_QUESTIONS] if MAX_QUESTIONS else data\n",
        "\n",
        "    for i, item in tqdm(enumerate(test_questions[already_processed:], start=already_processed),\n",
        "                         total=len(test_questions)-already_processed,\n",
        "                         desc=\"Processing questions\"):\n",
        "        question = item[\"Question\"]\n",
        "        original_answer = item[\"Answer\"]\n",
        "        print(f\"\\n[{i+1}/{len(test_questions)}] Processing question\")\n",
        "\n",
        "        result = experiment_semantic_entropy(\n",
        "            question=question,\n",
        "            original_answer=original_answer,\n",
        "            llm_model=llm_model,\n",
        "            llm_tokenizer=llm_tokenizer,\n",
        "            entailment_model=entailment_model,\n",
        "            num_samples=10\n",
        "        )\n",
        "        results.append(result)\n",
        "\n",
        "        # Save progress after every 5 questions\n",
        "        if (i+1) % 5 == 0:\n",
        "            with open(partial_save_path, 'w', encoding='utf-8') as f:\n",
        "                json.dump(results, f, ensure_ascii=False, indent=2)\n",
        "            print(f\"✓ Saved partial progress after {i+1} questions.\")\n",
        "\n",
        "    # Save final results\n",
        "    final_save_path = f'semantic_entropy_{MODEL_CHOICE}_{DATASET_CHOICE}_results.json'\n",
        "    print(\"\\nSaving final results...\")\n",
        "    with open(final_save_path, 'w', encoding='utf-8') as f:\n",
        "        json.dump(results, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "\n",
        "    files.download(final_save_path)\n",
        "    print(f\"✓ Experiment completed successfully with {MODEL_CHOICE} model!\")\n"
      ],
      "metadata": {
        "id": "7kc_fQg8hzjI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRo1E9seb8tM"
      },
      "source": [
        "# **extra**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ht7O3pP9bGFU"
      },
      "outputs": [],
      "source": [
        "files.download(f'semantic_entropy_{MODEL_CHOICE}_{DATASET_CHOICE}_results.json')\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyPCP1SZVzYHD58MK+am/asD",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}