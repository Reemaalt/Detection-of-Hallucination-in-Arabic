{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "txozLEPdYoIS",
        "outputId": "ddc2e8cc-c81f-4b5b-baf1-1b88fdf31e41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluation Metrics:\n",
            "mutual_information_auroc: 0.1557\n",
            "predictive_entropy_auroc: 0.6868\n",
            "entropy_over_concepts_auroc: 0.6688\n",
            "margin_probability_auroc: 0.6529\n",
            "semantic_entropy_auroc: 0.6774\n",
            "\n",
            "Matched 708 out of 708 questions with hallucination labels.\n",
            "\n",
            "ROC curve saved as 'roc_curves.png'.\n",
            "\n",
            "Summary Statistics for Correct vs Hallucinated Responses:\n",
            "mutual_information:\n",
            "  Non-hallucinated mean: 0.2262\n",
            "  Hallucinated mean: 0.1309\n",
            "  Difference: -42.1%\n",
            "predictive_entropy:\n",
            "  Non-hallucinated mean: 0.0172\n",
            "  Hallucinated mean: 0.1431\n",
            "  Difference: +729.9%\n",
            "entropy_over_concepts:\n",
            "  Non-hallucinated mean: 0.0037\n",
            "  Hallucinated mean: 0.0188\n",
            "  Difference: +405.0%\n",
            "margin_probability:\n",
            "  Non-hallucinated mean: 0.9920\n",
            "  Hallucinated mean: 0.8857\n",
            "  Difference: -10.7%\n",
            "semantic_entropy:\n",
            "  Non-hallucinated mean: 0.0172\n",
            "  Hallucinated mean: 0.1431\n",
            "  Difference: +729.9%\n",
            "\n",
            "Analysis complete! Results saved to CSV and ROC curves saved as 'roc_curves.png'.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "\n",
        "# Load JSON data\n",
        "with open(\"/content/semantic_entropy_Llama3.1-8b_xor_tydiqa_results.json\", 'r') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "with open(\"/content/labeled_data_XORfull_rougel_isri.json\", 'r') as f:\n",
        "    hallucination_labels = json.load(f)\n",
        "\n",
        "# Small epsilon value to avoid log(0)\n",
        "epsilon = 1e-10\n",
        "\n",
        "def compute_mutual_information(cluster_probabilities):\n",
        "    cluster_probs = torch.tensor([float(x) for x in cluster_probabilities], dtype=torch.float32)\n",
        "    variance = torch.var(cluster_probs, dim=0)\n",
        "    return torch.mean(variance)\n",
        "\n",
        "def compute_predictive_entropy(cluster_probabilities):\n",
        "    cluster_probs = torch.tensor([float(x) for x in cluster_probabilities], dtype=torch.float32) + epsilon\n",
        "    entropy = -torch.sum(cluster_probs * torch.log(cluster_probs))\n",
        "    return entropy\n",
        "\n",
        "def compute_entropy_over_concepts(cluster_probabilities, semantic_set_ids):\n",
        "    cluster_probs = torch.tensor([float(x) for x in cluster_probabilities], dtype=torch.float32) + epsilon\n",
        "    if not semantic_set_ids:\n",
        "        return compute_predictive_entropy(cluster_probabilities)\n",
        "\n",
        "    unique_concepts = torch.unique(torch.tensor(semantic_set_ids, dtype=torch.int64))\n",
        "    entropies = []\n",
        "    for concept in unique_concepts:\n",
        "        concept_probs = cluster_probs[torch.tensor(semantic_set_ids, dtype=torch.int64) == concept]\n",
        "        entropy = -torch.sum(concept_probs * torch.log(concept_probs))\n",
        "        entropies.append(entropy)\n",
        "\n",
        "    return torch.mean(torch.stack(entropies)) if entropies else compute_predictive_entropy(cluster_probabilities)\n",
        "\n",
        "def compute_margin_probability(cluster_probabilities):\n",
        "    cluster_probs = torch.tensor([float(x) for x in cluster_probabilities], dtype=torch.float32)\n",
        "    sorted_probs, _ = torch.sort(cluster_probs, descending=True)\n",
        "    return sorted_probs[0] - sorted_probs[1] if len(sorted_probs) > 1 else torch.tensor(1.0)\n",
        "\n",
        "# Evaluate AUROC metrics\n",
        "def evaluate_uncertainty_metrics(results_df):\n",
        "    metrics = {}\n",
        "    try:\n",
        "        metrics['mutual_information_auroc'] = roc_auc_score(1 - results_df['correct'], results_df['mutual_information'])\n",
        "        metrics['predictive_entropy_auroc'] = roc_auc_score(1 - results_df['correct'], results_df['predictive_entropy'])\n",
        "        metrics['entropy_over_concepts_auroc'] = roc_auc_score(1 - results_df['correct'], results_df['entropy_over_concepts'])\n",
        "        metrics['margin_probability_auroc'] = roc_auc_score(1 - results_df['correct'], -results_df['margin_probability'])\n",
        "        metrics['semantic_entropy_auroc'] = roc_auc_score(1 - results_df['correct'], results_df['semantic_entropy'])\n",
        "    except ValueError as e:\n",
        "        print(f\"Error computing AUROC: {e}\")\n",
        "    return metrics\n",
        "\n",
        "# Function to plot and save AUROC curves\n",
        "def plot_roc_curves(results_df):\n",
        "    plt.figure(figsize=(10, 8))\n",
        "\n",
        "    metrics = {\n",
        "        'mutual_information': results_df['mutual_information'],\n",
        "        'predictive_entropy': results_df['predictive_entropy'],\n",
        "        'entropy_over_concepts': results_df['entropy_over_concepts'],\n",
        "        'margin_probability': -results_df['margin_probability'],  # Inverted\n",
        "        'semantic_entropy': results_df['semantic_entropy']\n",
        "    }\n",
        "\n",
        "    for name, values in metrics.items():\n",
        "        fpr, tpr, _ = roc_curve(1 - results_df['correct'], values)\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "        plt.plot(fpr, tpr, label=f'{name} (AUC = {roc_auc:.3f})')\n",
        "\n",
        "    plt.plot([0, 1], [0, 1], 'k--')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('ROC Curves for Hallucination Detection')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.savefig('roc_curves.png')  # Save AUROC plot\n",
        "    plt.close()\n",
        "    print(\"\\nROC curve saved as 'roc_curves.png'.\")\n",
        "\n",
        "# Prepare results\n",
        "results = []\n",
        "matched_count = 0\n",
        "\n",
        "# Match hallucination labels with uncertainty data\n",
        "for item_id, item in data.items():\n",
        "    question = item['question']\n",
        "    cluster_probabilities = item.get('cluster_probabilities', [])\n",
        "\n",
        "    # Ensure cluster_probabilities is valid\n",
        "    if not cluster_probabilities or not all(isinstance(x, (int, float)) for x in cluster_probabilities):\n",
        "        print(f\"Skipping item {item_id} due to invalid cluster_probabilities.\")\n",
        "        continue\n",
        "\n",
        "    # Generate semantic_set_ids if missing\n",
        "    num_clusters = item.get('num_clusters', len(cluster_probabilities))\n",
        "    semantic_set_ids = list(range(num_clusters))\n",
        "\n",
        "    # Compute uncertainty measures\n",
        "    try:\n",
        "        mutual_info = compute_mutual_information(cluster_probabilities)\n",
        "        predictive_entropy = compute_predictive_entropy(cluster_probabilities)\n",
        "        entropy_over_concepts = compute_entropy_over_concepts(cluster_probabilities, semantic_set_ids)\n",
        "        margin_probability = compute_margin_probability(cluster_probabilities)\n",
        "        semantic_entropy = item.get('semantic_entropy', np.nan)\n",
        "\n",
        "        # Check if the item exists in hallucination_labels\n",
        "        if item_id in hallucination_labels:\n",
        "            question_label = hallucination_labels[item_id].get('computed_question_label', \"Unknown\")\n",
        "            is_correct = 0 if question_label == \"Hallucinated\" else 1\n",
        "            matched_count += 1\n",
        "\n",
        "            results.append({\n",
        "                'id': item_id,\n",
        "                'question': question,\n",
        "                'mutual_information': mutual_info.item(),\n",
        "                'predictive_entropy': predictive_entropy.item(),\n",
        "                'entropy_over_concepts': entropy_over_concepts.item(),\n",
        "                'margin_probability': margin_probability.item(),\n",
        "                'semantic_entropy': semantic_entropy,\n",
        "                'correct': is_correct,\n",
        "                'hallucination_status': question_label\n",
        "            })\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing item {item_id}: {e}\")\n",
        "\n",
        "# Convert results to DataFrame\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df.fillna(0, inplace=True)  # Handle NaN values\n",
        "\n",
        "# Compute AUROC metrics\n",
        "evaluation_metrics = evaluate_uncertainty_metrics(results_df)\n",
        "print(\"\\nEvaluation Metrics:\")\n",
        "for metric, value in evaluation_metrics.items():\n",
        "    print(f\"{metric}: {value:.4f}\")\n",
        "\n",
        "# Save results\n",
        "results_df.to_csv('uncertainty_hallucination_metrics.csv', index=False)\n",
        "with open('uncertainty_evaluation_metrics.json', 'w') as f:\n",
        "    json.dump(evaluation_metrics, f, indent=4)\n",
        "\n",
        "print(f\"\\nMatched {matched_count} out of {len(data)} questions with hallucination labels.\")\n",
        "\n",
        "# Save AUROC plot\n",
        "plot_roc_curves(results_df)\n",
        "\n",
        "# Calculate and print mean values for correct vs incorrect responses\n",
        "print(\"\\nSummary Statistics for Correct vs Hallucinated Responses:\")\n",
        "metrics = ['mutual_information', 'predictive_entropy', 'entropy_over_concepts', 'margin_probability', 'semantic_entropy']\n",
        "for metric in metrics:\n",
        "    non_hallucinated_mean = results_df[results_df['correct'] == 1][metric].mean()\n",
        "    hallucinated_mean = results_df[results_df['correct'] == 0][metric].mean()\n",
        "    diff_percent = (hallucinated_mean - non_hallucinated_mean) / non_hallucinated_mean * 100\n",
        "    print(f\"{metric}:\")\n",
        "    print(f\"  Non-hallucinated mean: {non_hallucinated_mean:.4f}\")\n",
        "    print(f\"  Hallucinated mean: {hallucinated_mean:.4f}\")\n",
        "    print(f\"  Difference: {diff_percent:+.1f}%\")\n",
        "\n",
        "print(\"\\nAnalysis complete! Results saved to CSV and ROC curves saved as 'roc_curves.png'.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import json\n"
      ],
      "metadata": {
        "id": "fuUiIFlPm_uU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data from the JSON files\n",
        "with open(\"/content/semantic_entropy_Llama3.1-8b_xquadAll_results (1).json\", 'r') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "with open(\"/content/labeled_data_rougel.json\", 'r') as f:\n",
        "    hallucination_labels = json.load(f)\n"
      ],
      "metadata": {
        "id": "vIe9K4n6nDcN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# compute the uncertainty measures"
      ],
      "metadata": {
        "id": "eh3C5DxbnJDk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Small Epsilon to Avoid Log(0\n",
        "epsilon = 1e-10\n",
        "\n",
        "\n",
        "\n",
        "def compute_mutual_information(cluster_probabilities):\n",
        "    cluster_probs = torch.tensor(cluster_probabilities)\n",
        "    variance = torch.var(cluster_probs, dim=0)\n",
        "    mutual_information = torch.mean(variance)\n",
        "    return mutual_information\n",
        "\n",
        "\n",
        "def compute_predictive_entropy(cluster_probabilities):\n",
        "    cluster_probs = torch.tensor(cluster_probabilities) + epsilon\n",
        "    entropy = -torch.sum(cluster_probs * torch.log(cluster_probs))\n",
        "    return entropy\n",
        "\n",
        "\n",
        "\n",
        "def compute_entropy_over_concepts(cluster_probabilities, semantic_set_ids):\n",
        "    \"\"\"Compute semantic entropy by grouping by concepts.\"\"\"\n",
        "    cluster_probs = torch.tensor(cluster_probabilities) + epsilon\n",
        "    unique_concepts = torch.unique(torch.tensor(semantic_set_ids))\n",
        "    entropies = []\n",
        "\n",
        "    for concept in unique_concepts:\n",
        "        concept_probs = cluster_probs[torch.tensor(semantic_set_ids) == concept]\n",
        "        entropy = -torch.sum(concept_probs * torch.log(concept_probs))\n",
        "        entropies.append(entropy)\n",
        "\n",
        "    if len(entropies) == 0:\n",
        "        return compute_predictive_entropy(cluster_probabilities)\n",
        "\n",
        "    return torch.mean(torch.stack(entropies))\n",
        "\n",
        "\n",
        "def compute_margin_probability(cluster_probabilities):\n",
        "    cluster_probs = torch.tensor(cluster_probabilities)\n",
        "    sorted_probs, _ = torch.sort(cluster_probs, descending=True)\n",
        "    if len(sorted_probs) == 1:\n",
        "        return torch.tensor(1.0)\n",
        "    margin = sorted_probs[0] - sorted_probs[1]\n",
        "    return margin\n",
        "\n"
      ],
      "metadata": {
        "id": "s6mClSlTnIaQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate Uncertainty Metrics + Plot ROC Curves"
      ],
      "metadata": {
        "id": "W0C9nPWtntrN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_uncertainty_metrics(results_df):\n",
        "    metrics = {}\n",
        "    metrics['mutual_information_auroc'] = roc_auc_score(1 - results_df['correct'], results_df['mutual_information'])\n",
        "    metrics['predictive_entropy_auroc'] = roc_auc_score(1 - results_df['correct'], results_df['predictive_entropy'])\n",
        "    metrics['entropy_over_concepts_auroc'] = roc_auc_score(1 - results_df['correct'], results_df['entropy_over_concepts'])\n",
        "    metrics['margin_probability_auroc'] = roc_auc_score(1 - results_df['correct'], -results_df['margin_probability'])\n",
        "    metrics['semantic_entropy_auroc'] = roc_auc_score(1 - results_df['correct'], results_df['semantic_entropy'])\n",
        "\n",
        "    return metrics\n",
        "\n",
        "\n",
        "def plot_roc_curves(results_df):\n",
        "    \"\"\"Plots ROC curves for different uncertainty metrics and saves as PNG.\"\"\"\n",
        "    plt.figure(figsize=(10, 8))\n",
        "\n",
        "    metrics = {\n",
        "        'mutual_information': results_df['mutual_information'],\n",
        "        'predictive_entropy': results_df['predictive_entropy'],\n",
        "        'entropy_over_concepts': results_df['entropy_over_concepts'],\n",
        "        'margin_probability': -results_df['margin_probability'],\n",
        "        'semantic_entropy': results_df['semantic_entropy']\n",
        "    }\n",
        "\n",
        "    for name, values in metrics.items():\n",
        "        fpr, tpr, _ = roc_curve(1 - results_df['correct'], values)\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "        plt.plot(fpr, tpr, label=f'{name} (AUC = {roc_auc:.3f})')\n",
        "\n",
        "    plt.plot([0, 1], [0, 1], 'k--')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('ROC Curves for Hallucination Detection')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.savefig('roc_curves.png')  # Save the AUROC plot\n",
        "    plt.close()\n",
        "    print(\"\\nROC curve saved as 'roc_curves.png'.\")\n"
      ],
      "metadata": {
        "id": "Zd_veZ1knbmS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Match Hallucination Labels & Computed Uncertainty Metrics"
      ],
      "metadata": {
        "id": "ZEUEi1fen20F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare to store results\n",
        "results = []\n",
        "\n",
        "# Match hallucination labels with uncertainty data\n",
        "matched_count = 0\n",
        "total_questions = len(data)\n",
        "\n",
        "for item_id, item in data.items():\n",
        "    question = item['question']\n",
        "    cluster_probabilities = item['cluster_probabilities']\n",
        "\n",
        "    # Generate semantic_set_ids since it's missing in the file\n",
        "    semantic_set_ids = list(range(item.get('num_clusters', len(cluster_probabilities))))\n",
        "\n",
        "    # Compute uncertainty measures\n",
        "    mutual_info = compute_mutual_information(cluster_probabilities)\n",
        "    predictive_entropy = compute_predictive_entropy(cluster_probabilities)\n",
        "    entropy_over_concepts = compute_entropy_over_concepts(cluster_probabilities, semantic_set_ids)\n",
        "    margin_probability = compute_margin_probability(cluster_probabilities)\n",
        "    semantic_entropy = item.get('semantic_entropy', 0)\n",
        "\n",
        "    # Match the hallucination label for each question from hallucination_labels\n",
        "    if item_id in hallucination_labels:\n",
        "        question_label = hallucination_labels[item_id]['question_label']\n",
        "        is_correct = 0 if question_label == \"Hallucinated\" else 1\n",
        "        matched_count += 1\n",
        "\n",
        "        # Store the results for this question\n",
        "        results.append({\n",
        "            'id': item_id,\n",
        "            'question': question,\n",
        "            'mutual_information': mutual_info.item(),\n",
        "            'predictive_entropy': predictive_entropy.item(),\n",
        "            'entropy_over_concepts': entropy_over_concepts.item(),\n",
        "            'margin_probability': margin_probability.item(),\n",
        "            'semantic_entropy': semantic_entropy,\n",
        "            'correct': is_correct,\n",
        "            'hallucination_status': question_label\n",
        "        })\n",
        "\n",
        "\n",
        "        # Handling NaN values in metrics computation\n",
        "for i, item in enumerate(results):\n",
        "    for metric in ['mutual_information', 'predictive_entropy', 'entropy_over_concepts', 'margin_probability', 'semantic_entropy']:\n",
        "        if metric not in item or math.isnan(item[metric]):\n",
        "            item[metric] = 0\n",
        "\n",
        "# Convert to DataFrame after cleaning NaNs\n",
        "results_df = pd.DataFrame(results)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S7p7j7FBn6O6",
        "outputId": "08ff8805-8003-4a10-8c20-c55a3fb8f0ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-14dbff0ca075>:8: UserWarning: var(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at /pytorch/aten/src/ATen/native/ReduceOps.cpp:1831.)\n",
            "  variance = torch.var(cluster_probs, dim=0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Results"
      ],
      "metadata": {
        "id": "0x7xFCLzoKZB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute and save AUROC values\n",
        "evaluation_metrics = evaluate_uncertainty_metrics(results_df)\n",
        "print(\"\\nEvaluation of Uncertainty Metrics:\")\n",
        "for metric, value in evaluation_metrics.items():\n",
        "    print(f\"{metric}: {value:.4f}\")\n",
        "\n",
        "# Print matching statistics\n",
        "print(f\"Successfully matched {matched_count} out of {total_questions} questions with hallucination labels.\")\n",
        "print(f\"Matching rate: {matched_count/total_questions:.2%}\")\n",
        "\n",
        "# Display summary statistics\n",
        "print(\"\\nSummary Statistics for Non-hallucinated vs Hallucinated:\")\n",
        "metrics = ['mutual_information', 'predictive_entropy', 'entropy_over_concepts', 'margin_probability', 'semantic_entropy']\n",
        "for metric in metrics:\n",
        "    non_hallucinated_mean = results_df[results_df['correct'] == 1][metric].mean()\n",
        "    hallucinated_mean = results_df[results_df['correct'] == 0][metric].mean()\n",
        "    diff_percent = (hallucinated_mean - non_hallucinated_mean) / non_hallucinated_mean * 100\n",
        "    print(f\"{metric}:\")\n",
        "    print(f\"  Non-hallucinated mean: {non_hallucinated_mean:.4f}\")\n",
        "    print(f\"  Hallucinated mean: {hallucinated_mean:.4f}\")\n",
        "    print(f\"  Difference: {diff_percent:+.1f}%\")\n",
        "\n",
        "    # Save AUROC plot\n",
        "plot_roc_curves(results_df)\n",
        "\n",
        "# Save results & metrics\n",
        "results_df.to_csv('uncertainty_hallucination_metrics.csv', index=False)\n",
        "with open('uncertainty_evaluation_metrics.json', 'w') as f:\n",
        "    json.dump(evaluation_metrics, f, indent=4)\n",
        "\n",
        "print(\"\\nAnalysis complete! Results saved.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HjGz28R4oDW3",
        "outputId": "d5b64fae-29dc-4fe9-907f-b60b721e54b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluation of Uncertainty Metrics:\n",
            "mutual_information_auroc: 0.2021\n",
            "predictive_entropy_auroc: 0.7979\n",
            "entropy_over_concepts_auroc: 0.5643\n",
            "margin_probability_auroc: 0.6151\n",
            "semantic_entropy_auroc: 0.7979\n",
            "Successfully matched 1190 out of 1190 questions with hallucination labels.\n",
            "Matching rate: 100.00%\n",
            "\n",
            "Summary Statistics for Non-hallucinated vs Hallucinated:\n",
            "mutual_information:\n",
            "  Non-hallucinated mean: 0.1473\n",
            "  Hallucinated mean: 0.0377\n",
            "  Difference: -74.4%\n",
            "predictive_entropy:\n",
            "  Non-hallucinated mean: 0.6991\n",
            "  Hallucinated mean: 1.2999\n",
            "  Difference: +85.9%\n",
            "entropy_over_concepts:\n",
            "  Non-hallucinated mean: 0.1541\n",
            "  Hallucinated mean: 0.1791\n",
            "  Difference: +16.2%\n",
            "margin_probability:\n",
            "  Non-hallucinated mean: 0.4348\n",
            "  Hallucinated mean: 0.2005\n",
            "  Difference: -53.9%\n",
            "semantic_entropy:\n",
            "  Non-hallucinated mean: 0.6991\n",
            "  Hallucinated mean: 1.2999\n",
            "  Difference: +85.9%\n",
            "\n",
            "ROC curve saved as 'roc_curves.png'.\n",
            "\n",
            "Analysis complete! Results saved.\n"
          ]
        }
      ]
    }
  ]
}