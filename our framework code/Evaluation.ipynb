{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n"
      ],
      "metadata": {
        "id": "CCqRVS3dipHb"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Load the uncertainty results and hallucination labels"
      ],
      "metadata": {
        "id": "X5JUbh-Bi6c7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/semantic_entropy_Llama3.1-8b_xor_tydiqa_results.json\", 'r') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "with open(\"/content/labeled_data_XORfull_rougel_isri.json\", 'r') as f:\n",
        "    hallucination_labels = json.load(f)\n"
      ],
      "metadata": {
        "id": "T3HsZV31ixd2"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Confidence measures functions"
      ],
      "metadata": {
        "id": "tiQMqsd6l32d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Small epsilon to avoid log(0)\n",
        "epsilon = 1e-10\n",
        "\n",
        "# Mutual Information = variance of cluster probabilities\n",
        "def compute_mutual_information(cluster_probabilities):\n",
        "    cluster_probs = torch.tensor([float(x) for x in cluster_probabilities], dtype=torch.float32)\n",
        "    variance = torch.var(cluster_probs, dim=0)\n",
        "    return torch.mean(variance)\n",
        "\n",
        "# Predictive Entropy\n",
        "def compute_predictive_entropy(cluster_probabilities):\n",
        "    cluster_probs = torch.tensor([float(x) for x in cluster_probabilities], dtype=torch.float32) + epsilon\n",
        "    entropy = -torch.sum(cluster_probs * torch.log(cluster_probs))\n",
        "    return entropy\n",
        "\n",
        "# Entropy over concepts (grouped entropy)\n",
        "def compute_entropy_over_concepts(cluster_probabilities, semantic_set_ids):\n",
        "    cluster_probs = torch.tensor([float(x) for x in cluster_probabilities], dtype=torch.float32) + epsilon\n",
        "    if not semantic_set_ids:\n",
        "        return compute_predictive_entropy(cluster_probabilities)\n",
        "\n",
        "    unique_concepts = torch.unique(torch.tensor(semantic_set_ids, dtype=torch.int64))\n",
        "    entropies = []\n",
        "    for concept in unique_concepts:\n",
        "        concept_probs = cluster_probs[torch.tensor(semantic_set_ids, dtype=torch.int64) == concept]\n",
        "        entropy = -torch.sum(concept_probs * torch.log(concept_probs))\n",
        "        entropies.append(entropy)\n",
        "\n",
        "    return torch.mean(torch.stack(entropies)) if entropies else compute_predictive_entropy(cluster_probabilities)\n",
        "\n",
        "# Margin Probability = top1 - top2 cluster confidence\n",
        "def compute_margin_probability(cluster_probabilities):\n",
        "    cluster_probs = torch.tensor([float(x) for x in cluster_probabilities], dtype=torch.float32)\n",
        "    sorted_probs, _ = torch.sort(cluster_probs, descending=True)\n",
        "    return sorted_probs[0] - sorted_probs[1] if len(sorted_probs) > 1 else torch.tensor(1.0)\n"
      ],
      "metadata": {
        "id": "d_bHSTbKjH3z"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## AUROC Calculation Function"
      ],
      "metadata": {
        "id": "8ss625-NjSb4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate AUROC for each uncertainty metric\n",
        "def evaluate_uncertainty_metrics(results_df):\n",
        "    metrics = {}\n",
        "    try:\n",
        "        metrics['mutual_information_auroc'] = roc_auc_score(1 - results_df['correct'], results_df['mutual_information'])\n",
        "        metrics['predictive_entropy_auroc'] = roc_auc_score(1 - results_df['correct'], results_df['predictive_entropy'])\n",
        "        metrics['entropy_over_concepts_auroc'] = roc_auc_score(1 - results_df['correct'], results_df['entropy_over_concepts'])\n",
        "        metrics['margin_probability_auroc'] = roc_auc_score(1 - results_df['correct'], -results_df['margin_probability'])\n",
        "        metrics['semantic_entropy_auroc'] = roc_auc_score(1 - results_df['correct'], results_df['semantic_entropy'])\n",
        "    except ValueError as e:\n",
        "        print(f\"Error computing AUROC: {e}\")\n",
        "    return metrics\n"
      ],
      "metadata": {
        "id": "lynYtVSDjdg9"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plotting Functions (ROC + Bar Chart)"
      ],
      "metadata": {
        "id": "fuhFcfrQjfuC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot ROC Curves for all metrics\n",
        "def plot_roc_curves(results_df):\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    metrics = {\n",
        "        'mutual_information': results_df['mutual_information'],\n",
        "        'predictive_entropy': results_df['predictive_entropy'],\n",
        "        'entropy_over_concepts': results_df['entropy_over_concepts'],\n",
        "        'margin_probability': -results_df['margin_probability'],  # flip for ROC\n",
        "        'semantic_entropy': results_df['semantic_entropy']\n",
        "    }\n",
        "    for name, values in metrics.items():\n",
        "        fpr, tpr, _ = roc_curve(1 - results_df['correct'], values)\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "        plt.plot(fpr, tpr, label=f'{name} (AUC = {roc_auc:.3f})')\n",
        "\n",
        "    plt.plot([0, 1], [0, 1], 'k--')\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('ROC Curves for Hallucination Detection')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.savefig('roc_curves.png')\n",
        "    plt.close()\n",
        "    print(\"✅ ROC curve saved as 'roc_curves.png'.\")\n",
        "\n",
        "# Plot AUROC bar chart with matching ROC curve colors\n",
        "def plot_auroc_bar_chart(auroc_dict, output_path='auroc_bar_chart.png'):\n",
        "    color_map = {\n",
        "        'mutual_information_auroc': '#1f77b4',\n",
        "        'predictive_entropy_auroc': '#ff7f0e',\n",
        "        'entropy_over_concepts_auroc': '#2ca02c',\n",
        "        'margin_probability_auroc': '#d62728',\n",
        "        'semantic_entropy_auroc': '#9467bd'\n",
        "    }\n",
        "\n",
        "    labels = list(auroc_dict.keys())\n",
        "    scores = [auroc_dict[label] for label in labels]\n",
        "    colors = [color_map.get(label, '#333333') for label in labels]\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    bars = plt.bar(labels, scores, color=colors)\n",
        "    plt.ylabel(\"AUROC Score\")\n",
        "    plt.title(\"Uncertainty Metrics - AUROC Comparison\")\n",
        "    plt.ylim(0, 1)\n",
        "\n",
        "    for bar, score in zip(bars, scores):\n",
        "        plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.01,\n",
        "                 f\"{score:.3f}\", ha='center', va='bottom')\n",
        "\n",
        "    plt.xticks(rotation=30, ha='right')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(output_path)\n",
        "    plt.close()\n",
        "    print(f\"✅ AUROC bar chart saved as '{output_path}'.\")\n"
      ],
      "metadata": {
        "id": "opNoGxs0jjYs"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run Everything (Main Script)"
      ],
      "metadata": {
        "id": "I27nyLPvjqvy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Process data and compute all metrics\n",
        "results = []\n",
        "matched_count = 0\n",
        "\n",
        "for item_id, item in data.items():\n",
        "    cluster_probabilities = item.get('cluster_probabilities', [])\n",
        "    if not cluster_probabilities or not all(isinstance(x, (int, float)) for x in cluster_probabilities):\n",
        "        continue\n",
        "\n",
        "    num_clusters = item.get('num_clusters', len(cluster_probabilities))\n",
        "    semantic_set_ids = list(range(num_clusters))\n",
        "\n",
        "    try:\n",
        "        mutual_info = compute_mutual_information(cluster_probabilities)\n",
        "        predictive_entropy = compute_predictive_entropy(cluster_probabilities)\n",
        "        entropy_over_concepts = compute_entropy_over_concepts(cluster_probabilities, semantic_set_ids)\n",
        "        margin_probability = compute_margin_probability(cluster_probabilities)\n",
        "        semantic_entropy = item.get('semantic_entropy', np.nan)\n",
        "\n",
        "        if item_id in hallucination_labels:\n",
        "            label = hallucination_labels[item_id].get('computed_question_label', \"Unknown\")\n",
        "            is_correct = 0 if label == \"Hallucinated\" else 1\n",
        "            matched_count += 1\n",
        "\n",
        "            results.append({\n",
        "                'id': item_id,\n",
        "                'question': item['question'],\n",
        "                'mutual_information': mutual_info.item(),\n",
        "                'predictive_entropy': predictive_entropy.item(),\n",
        "                'entropy_over_concepts': entropy_over_concepts.item(),\n",
        "                'margin_probability': margin_probability.item(),\n",
        "                'semantic_entropy': semantic_entropy,\n",
        "                'correct': is_correct,\n",
        "                'hallucination_status': label\n",
        "            })\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error processing {item_id}: {e}\")\n"
      ],
      "metadata": {
        "id": "_SGrk4DPjtdg"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save + Visualize Results"
      ],
      "metadata": {
        "id": "_wgm9vx5j1Dx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to DataFrame\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df.fillna(0, inplace=True)\n",
        "\n",
        "# Compute AUROC\n",
        "evaluation_metrics = evaluate_uncertainty_metrics(results_df)\n",
        "print(\"🎯 Evaluation Metrics:\")\n",
        "for k, v in evaluation_metrics.items():\n",
        "    print(f\"{k}: {v:.4f}\")\n",
        "\n",
        "# Save results to files\n",
        "results_df.to_csv('uncertainty_hallucination_metrics.csv', index=False)\n",
        "with open('uncertainty_evaluation_metrics.json', 'w') as f:\n",
        "    json.dump(evaluation_metrics, f, indent=4)\n",
        "\n",
        "print(f\"\\n✅ Matched {matched_count} out of {len(data)} items.\")\n",
        "\n",
        "# Plot\n",
        "plot_roc_curves(results_df)\n",
        "plot_auroc_bar_chart(evaluation_metrics)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k8GErilmj45s",
        "outputId": "7d726382-c553-4dae-db71-7cf2908fd2b0"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🎯 Evaluation Metrics:\n",
            "mutual_information_auroc: 0.1557\n",
            "predictive_entropy_auroc: 0.6868\n",
            "entropy_over_concepts_auroc: 0.6688\n",
            "margin_probability_auroc: 0.6529\n",
            "semantic_entropy_auroc: 0.6774\n",
            "\n",
            "✅ Matched 708 out of 708 items.\n",
            "✅ ROC curve saved as 'roc_curves.png'.\n",
            "✅ AUROC bar chart saved as 'auroc_bar_chart.png'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Print Summary Stats"
      ],
      "metadata": {
        "id": "5-ZI55-zj76J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print mean values for each metric by hallucination label\n",
        "metrics = ['mutual_information', 'predictive_entropy', 'entropy_over_concepts', 'margin_probability', 'semantic_entropy']\n",
        "print(\"\\n📊 Summary Stats (Correct vs Hallucinated):\")\n",
        "for metric in metrics:\n",
        "    non_hall = results_df[results_df['correct'] == 1][metric].mean()\n",
        "    hall = results_df[results_df['correct'] == 0][metric].mean()\n",
        "    diff = (hall - non_hall) / non_hall * 100\n",
        "    print(f\"{metric}:\")\n",
        "    print(f\"  ✅ Non-hallucinated: {non_hall:.4f}\")\n",
        "    print(f\"  ❌ Hallucinated:     {hall:.4f}\")\n",
        "    print(f\"  📉 Diff: {diff:+.1f}%\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uy3Yjx0Tj9Vf",
        "outputId": "c0dbe992-4c30-4486-8875-bec095ba6bfd"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📊 Summary Stats (Correct vs Hallucinated):\n",
            "mutual_information:\n",
            "  ✅ Non-hallucinated: 0.2262\n",
            "  ❌ Hallucinated:     0.1309\n",
            "  📉 Diff: -42.1%\n",
            "\n",
            "predictive_entropy:\n",
            "  ✅ Non-hallucinated: 0.0172\n",
            "  ❌ Hallucinated:     0.1431\n",
            "  📉 Diff: +729.9%\n",
            "\n",
            "entropy_over_concepts:\n",
            "  ✅ Non-hallucinated: 0.0037\n",
            "  ❌ Hallucinated:     0.0188\n",
            "  📉 Diff: +405.0%\n",
            "\n",
            "margin_probability:\n",
            "  ✅ Non-hallucinated: 0.9920\n",
            "  ❌ Hallucinated:     0.8857\n",
            "  📉 Diff: -10.7%\n",
            "\n",
            "semantic_entropy:\n",
            "  ✅ Non-hallucinated: 0.0172\n",
            "  ❌ Hallucinated:     0.1431\n",
            "  📉 Diff: +729.9%\n",
            "\n"
          ]
        }
      ]
    }
  ]
}