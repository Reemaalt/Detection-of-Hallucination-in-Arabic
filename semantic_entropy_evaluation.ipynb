{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPWUfH9eBVgTGf8hypfjp7h",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Reemaalt/Detection-of-Hallucination-in-Arabic/blob/main/semantic_entropy_evaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RougeL\n"
      ],
      "metadata": {
        "id": "2gsGU1b3FSL1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "klTh1158Do8w"
      },
      "outputs": [],
      "source": [
        "! pip install rouge-score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer\n",
        "from rouge_score import rouge_scorer\n",
        "from sklearn.metrics import roc_auc_score"
      ],
      "metadata": {
        "id": "J5Wq0RHvEAhk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load computed semantic entropy values\n",
        "\n",
        "entropy_file = \"s.json\"\n",
        "\n",
        "with open(entropy_file, \"r\", encoding=\"utf-8\") as f:\n",
        "    entropy_data = json.load(f)\n",
        "\n",
        "# Extract question IDs and entropy scores\n",
        "question_ids = list(entropy_data.keys())\n",
        "semantic_entropy_scores = [entropy_data[q][\"semantic_entropy\"] for q in question_ids]\n"
      ],
      "metadata": {
        "id": "4Zh9P41nDs1d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load AraBERT tokenizer\n",
        "model_name = 'aubmindlab/bert-base-arabertv02'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Initialize the ROUGE scorer\n",
        "scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n"
      ],
      "metadata": {
        "id": "xNIugipvD6dQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to save labels to a new JSON file\n",
        "def save_labels(output_data, output_file_path):\n",
        "    with open(output_file_path, 'w', encoding='utf-8') as file:\n",
        "        json.dump(output_data, file, ensure_ascii=False, indent=4)\n",
        "\n",
        "# Function to calculate ROUGE score and check hallucination\n",
        "def check_hallucination(data):\n",
        "    output_data = []  # List to store the labeled data\n",
        "\n",
        "    for item in data:\n",
        "        question_id = item['question_id']\n",
        "        question = item['question']\n",
        "        original_answer = item['original_answer']['text'][0]  # Get the first text of the original answer\n",
        "\n",
        "        # Dictionary to store results for each question\n",
        "        question_data = {\n",
        "            \"question_id\": question_id,\n",
        "            \"question\": question,\n",
        "            \"original_answer\": item['original_answer'],  # Keep original answer as it is\n",
        "            \"generated_answers\": []\n",
        "        }\n",
        "\n",
        "        for generated_answer in item['generated_answers']:\n",
        "            # Calculate ROUGE-L F1 score\n",
        "            scores = scorer.score(original_answer, generated_answer)\n",
        "            rouge_l_f1 = scores['rougeL'].fmeasure\n",
        "\n",
        "            # Check if hallucinated or not\n",
        "            if rouge_l_f1 < 0.3:\n",
        "                status = \"Hallucinated\"\n",
        "            else:\n",
        "                status = \"Non-Hallucinated\"\n",
        "\n",
        "            # Append the labeled answer with F1 score to the question data\n",
        "            question_data[\"generated_answers\"].append({\n",
        "                \"text\": generated_answer,\n",
        "                \"f1_score\": round(rouge_l_f1, 2),  # Round to 2 decimal places\n",
        "                \"label\": status\n",
        "            })\n",
        "\n",
        "        # Append the question data to the output list\n",
        "        output_data.append(question_data)\n",
        "\n",
        "    return output_data"
      ],
      "metadata": {
        "id": "0-hIl9yQE5Bd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the function and get labeled data\n",
        "\n",
        "hallucination_input_file = \"generated_responses.json\"  # Change to actual filename\n",
        "with open(hallucination_input_file, \"r\", encoding=\"utf-8\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "labeled_data = check_hallucination(data)\n",
        "\n",
        "# Save the labeled data to a new JSON file\n",
        "output_file_path = 'labeled_data.json'\n",
        "save_labels(labeled_data, output_file_path)\n",
        "print(f\"Labeled data has been saved to {output_file_path}\")"
      ],
      "metadata": {
        "id": "T33W78AqExAn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
"    # Step 6: get results"        
"# Compute AUROC"
      ],
      "metadata": {
        "id": "OiFuFetAFZQz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load correctness labels (1 = Correct, 0 = Incorrect)\n",
        "correctness_file = \"correct_labels.json\"\n",
        "\n",
        "with open(correctness_file, \"r\", encoding=\"utf-8\") as f:\n",
        "    correctness_data = json.load(f)\n",
        "\n",
        "# Match correctness labels with entropy scores\n",
        "correct_labels = [correctness_data.get(q, 0) for q in question_ids]  # Default to 0 if missing\n",
        "\n",
        "# Compute AUROC (Higher entropy should align with incorrect answers)\n",
        "auroc = roc_auc_score(1 - np.array(correct_labels), np.array(semantic_entropy_scores))\n",
        "\n",
        "print(f\"Semantic Entropy AUROC: {auroc:.4f}\")"
      ],
      "metadata": {
        "id": "f8gqFKpTFFE4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
